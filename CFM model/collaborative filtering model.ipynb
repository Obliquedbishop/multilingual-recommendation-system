{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b639833",
   "metadata": {},
   "source": [
    "## Installations 🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c779291c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aicrowd-cli\n",
      "  Using cached aicrowd_cli-0.1.15-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: click<8,>=7.1.2 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (7.1.2)\n",
      "Collecting GitPython==3.1.18 (from aicrowd-cli)\n",
      "  Using cached GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "Requirement already satisfied: requests<3,>=2.25.1 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (2.26.0)\n",
      "Collecting requests-toolbelt<1,>=0.9.1 (from aicrowd-cli)\n",
      "  Using cached requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: rich<11,>=10.0.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (10.16.2)\n",
      "Requirement already satisfied: toml<1,>=0.10.2 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (0.10.2)\n",
      "Requirement already satisfied: tqdm<5,>=4.56.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (4.64.0)\n",
      "Requirement already satisfied: pyzmq==22.1.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (22.1.0)\n",
      "Collecting python-slugify<6,>=5.0.0 (from aicrowd-cli)\n",
      "  Using cached python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: semver<3,>=2.13.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aicrowd-cli) (2.13.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython==3.1.18->aicrowd-cli)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyarrow) (1.23.5)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-slugify<6,>=5.0.0->aicrowd-cli) (1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.25.1->aicrowd-cli) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.25.1->aicrowd-cli) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.25.1->aicrowd-cli) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.25.1->aicrowd-cli) (3.1)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich<11,>=10.0.0->aicrowd-cli) (0.4.4)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich<11,>=10.0.0->aicrowd-cli) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich<11,>=10.0.0->aicrowd-cli) (2.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\shreyansh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython==3.1.18->aicrowd-cli) (5.0.0)\n",
      "Installing collected packages: python-slugify, gitdb, requests-toolbelt, GitPython, aicrowd-cli\n",
      "  Attempting uninstall: python-slugify\n",
      "    Found existing installation: python-slugify 6.1.1\n",
      "    Uninstalling python-slugify-6.1.1:\n",
      "      Successfully uninstalled python-slugify-6.1.1\n",
      "Successfully installed GitPython-3.1.18 aicrowd-cli-0.1.15 gitdb-4.0.10 python-slugify-5.0.2 requests-toolbelt-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install aicrowd-cli pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccb516",
   "metadata": {},
   "source": [
    "## Login to AIcrowd and download the data 📚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2362db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please login here: https://api.aicrowd.com/auth/cvcRrzkcxOAtk3lrt71t4Ld-M9S2kKBQ2NKrN93ofAg\n",
      "API Key valid\n",
      "Gitlab access token valid\n",
      "Saved details successfully!\n"
     ]
    }
   ],
   "source": [
    "!aicrowd login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854d2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sessions_test_task1_phase1.csv:   0%|          | 0.00/19.4M [00:00<?, ?B/s]\n",
      "sessions_test_task1_phase1.csv:   5%|5         | 1.05M/19.4M [00:01<00:27, 673kB/s]\n",
      "sessions_test_task1_phase1.csv:  11%|#         | 2.10M/19.4M [00:02<00:19, 883kB/s]\n",
      "sessions_test_task1_phase1.csv:  22%|##1       | 4.19M/19.4M [00:03<00:10, 1.48MB/s]\n",
      "sessions_test_task1_phase1.csv:  32%|###2      | 6.29M/19.4M [00:04<00:08, 1.55MB/s]\n",
      "sessions_test_task1_phase1.csv:  38%|###7      | 7.34M/19.4M [00:05<00:07, 1.63MB/s]\n",
      "sessions_test_task1_phase1.csv:  43%|####3     | 8.39M/19.4M [00:05<00:05, 1.83MB/s]\n",
      "sessions_test_task1_phase1.csv:  49%|####8     | 9.44M/19.4M [00:05<00:05, 1.96MB/s]\n",
      "sessions_test_task1_phase1.csv:  54%|#####4    | 10.5M/19.4M [00:06<00:04, 2.18MB/s]\n",
      "sessions_test_task1_phase1.csv:  60%|#####9    | 11.5M/19.4M [00:06<00:03, 2.37MB/s]\n",
      "sessions_test_task1_phase1.csv:  65%|######4   | 12.6M/19.4M [00:07<00:02, 2.45MB/s]\n",
      "sessions_test_task1_phase1.csv:  70%|#######   | 13.6M/19.4M [00:07<00:02, 2.62MB/s]\n",
      "sessions_test_task1_phase1.csv:  76%|#######5  | 14.7M/19.4M [00:07<00:01, 2.72MB/s]\n",
      "sessions_test_task1_phase1.csv:  81%|########1 | 15.7M/19.4M [00:08<00:01, 2.84MB/s]\n",
      "sessions_test_task1_phase1.csv:  87%|########6 | 16.8M/19.4M [00:08<00:01, 2.31MB/s]\n",
      "sessions_test_task1_phase1.csv:  92%|#########2| 17.8M/19.4M [00:09<00:00, 2.50MB/s]\n",
      "sessions_test_task1_phase1.csv:  97%|#########7| 18.9M/19.4M [00:09<00:00, 2.58MB/s]\n",
      "sessions_test_task1_phase1.csv: 100%|##########| 19.4M/19.4M [00:10<00:00, 1.83MB/s]\n",
      "sessions_test_task1_phase1.csv: 100%|##########| 19.4M/19.4M [00:10<00:00, 1.92MB/s]\n",
      "\n",
      "sessions_test_task2_phase1.csv:   0%|          | 0.00/1.92M [00:00<?, ?B/s]\n",
      "sessions_test_task2_phase1.csv:  55%|#####4    | 1.05M/1.92M [00:01<00:01, 685kB/s]\n",
      "sessions_test_task2_phase1.csv: 100%|##########| 1.92M/1.92M [00:01<00:00, 1.10MB/s]\n",
      "sessions_test_task2_phase1.csv: 100%|##########| 1.92M/1.92M [00:01<00:00, 993kB/s] \n",
      "\n",
      "sessions_test_task3_phase1.csv:   0%|          | 0.00/2.67M [00:00<?, ?B/s]\n",
      "sessions_test_task3_phase1.csv:  39%|###9      | 1.05M/2.67M [00:01<00:02, 575kB/s]\n",
      "sessions_test_task3_phase1.csv:  78%|#######8  | 2.10M/2.67M [00:02<00:00, 1.03MB/s]\n",
      "sessions_test_task3_phase1.csv: 100%|##########| 2.67M/2.67M [00:02<00:00, 1.34MB/s]\n",
      "sessions_test_task3_phase1.csv: 100%|##########| 2.67M/2.67M [00:02<00:00, 1.09MB/s]\n",
      "\n",
      "sessions_test_task1.csv:   0%|          | 0.00/19.3M [00:00<?, ?B/s]\n",
      "sessions_test_task1.csv:   5%|5         | 1.05M/19.3M [00:01<00:29, 621kB/s]\n",
      "sessions_test_task1.csv:  11%|#         | 2.10M/19.3M [00:02<00:15, 1.08MB/s]\n",
      "sessions_test_task1.csv:  16%|#6        | 3.15M/19.3M [00:02<00:10, 1.57MB/s]\n",
      "sessions_test_task1.csv:  22%|##1       | 4.19M/19.3M [00:03<00:13, 1.12MB/s]\n",
      "sessions_test_task1.csv:  27%|##7       | 5.24M/19.3M [00:04<00:10, 1.29MB/s]\n",
      "sessions_test_task1.csv:  33%|###2      | 6.29M/19.3M [00:05<00:11, 1.12MB/s]\n",
      "sessions_test_task1.csv:  38%|###8      | 7.34M/19.3M [00:05<00:07, 1.52MB/s]\n",
      "sessions_test_task1.csv:  43%|####3     | 8.39M/19.3M [00:06<00:05, 1.89MB/s]\n",
      "sessions_test_task1.csv:  49%|####8     | 9.44M/19.3M [00:06<00:04, 2.20MB/s]\n",
      "sessions_test_task1.csv:  54%|#####4    | 10.5M/19.3M [00:06<00:03, 2.37MB/s]\n",
      "sessions_test_task1.csv:  60%|#####9    | 11.5M/19.3M [00:07<00:03, 2.43MB/s]\n",
      "sessions_test_task1.csv:  65%|######5   | 12.6M/19.3M [00:08<00:04, 1.61MB/s]\n",
      "sessions_test_task1.csv:  71%|#######   | 13.6M/19.3M [00:08<00:02, 2.09MB/s]\n",
      "sessions_test_task1.csv:  76%|#######6  | 14.7M/19.3M [00:10<00:03, 1.26MB/s]\n",
      "sessions_test_task1.csv:  82%|########1 | 15.7M/19.3M [00:10<00:02, 1.61MB/s]\n",
      "sessions_test_task1.csv:  87%|########6 | 16.8M/19.3M [00:10<00:01, 1.60MB/s]\n",
      "sessions_test_task1.csv:  92%|#########2| 17.8M/19.3M [00:12<00:01, 1.17MB/s]\n",
      "sessions_test_task1.csv:  98%|#########7| 18.9M/19.3M [00:12<00:00, 1.50MB/s]\n",
      "sessions_test_task1.csv: 100%|##########| 19.3M/19.3M [00:12<00:00, 1.60MB/s]\n",
      "sessions_test_task1.csv: 100%|##########| 19.3M/19.3M [00:12<00:00, 1.51MB/s]\n",
      "\n",
      "sessions_test_task2.csv:   0%|          | 0.00/1.91M [00:00<?, ?B/s]\n",
      "sessions_test_task2.csv:  55%|#####4    | 1.05M/1.91M [00:03<00:02, 312kB/s]\n",
      "sessions_test_task2.csv: 100%|##########| 1.91M/1.91M [00:04<00:00, 522kB/s]\n",
      "sessions_test_task2.csv: 100%|##########| 1.91M/1.91M [00:04<00:00, 467kB/s]\n",
      "\n",
      "sessions_test_task3.csv:   0%|          | 0.00/2.67M [00:00<?, ?B/s]\n",
      "sessions_test_task3.csv:  39%|###9      | 1.05M/2.67M [00:01<00:02, 632kB/s]\n",
      "sessions_test_task3.csv:  79%|#######8  | 2.10M/2.67M [00:02<00:00, 1.07MB/s]\n",
      "sessions_test_task3.csv: 100%|##########| 2.67M/2.67M [00:02<00:00, 1.11MB/s]\n",
      "sessions_test_task3.csv: 100%|##########| 2.67M/2.67M [00:02<00:00, 1.00MB/s]\n",
      "\n",
      "products_train.csv:   0%|          | 0.00/589M [00:00<?, ?B/s]\n",
      "products_train.csv:   0%|          | 1.05M/589M [00:01<15:09, 646kB/s]\n",
      "products_train.csv:   0%|          | 2.10M/589M [00:02<08:39, 1.13MB/s]\n",
      "products_train.csv:   1%|          | 3.15M/589M [00:02<06:02, 1.62MB/s]\n",
      "products_train.csv:   1%|          | 4.19M/589M [00:03<07:20, 1.33MB/s]\n",
      "products_train.csv:   1%|          | 5.24M/589M [00:04<07:09, 1.36MB/s]\n",
      "products_train.csv:   1%|1         | 6.29M/589M [00:04<05:02, 1.92MB/s]\n",
      "products_train.csv:   1%|1         | 7.34M/589M [00:04<04:18, 2.25MB/s]\n",
      "products_train.csv:   1%|1         | 8.39M/589M [00:04<04:00, 2.42MB/s]\n",
      "products_train.csv:   2%|1         | 9.44M/589M [00:05<03:49, 2.52MB/s]\n",
      "products_train.csv:   2%|1         | 10.5M/589M [00:06<05:33, 1.73MB/s]\n",
      "products_train.csv:   2%|1         | 11.5M/589M [00:07<05:53, 1.63MB/s]\n",
      "products_train.csv:   2%|2         | 12.6M/589M [00:07<05:09, 1.86MB/s]\n",
      "products_train.csv:   2%|2         | 13.6M/589M [00:07<04:36, 2.08MB/s]\n",
      "products_train.csv:   2%|2         | 14.7M/589M [00:08<04:04, 2.35MB/s]\n",
      "products_train.csv:   3%|2         | 15.7M/589M [00:08<05:18, 1.80MB/s]\n",
      "products_train.csv:   3%|2         | 16.8M/589M [00:09<04:33, 2.09MB/s]\n",
      "products_train.csv:   3%|3         | 17.8M/589M [00:10<05:28, 1.74MB/s]\n",
      "products_train.csv:   3%|3         | 18.9M/589M [00:10<05:06, 1.86MB/s]\n",
      "products_train.csv:   3%|3         | 19.9M/589M [00:11<05:06, 1.86MB/s]\n",
      "products_train.csv:   4%|3         | 21.0M/589M [00:11<05:14, 1.80MB/s]\n",
      "products_train.csv:   4%|3         | 22.0M/589M [00:12<05:19, 1.77MB/s]\n",
      "products_train.csv:   4%|3         | 23.1M/589M [00:13<05:23, 1.75MB/s]\n",
      "products_train.csv:   4%|4         | 24.1M/589M [00:13<05:24, 1.74MB/s]\n",
      "products_train.csv:   4%|4         | 25.2M/589M [00:14<05:26, 1.73MB/s]\n",
      "products_train.csv:   4%|4         | 26.2M/589M [00:14<05:27, 1.72MB/s]\n",
      "products_train.csv:   5%|4         | 27.3M/589M [00:15<05:27, 1.71MB/s]\n",
      "products_train.csv:   5%|4         | 28.3M/589M [00:16<05:26, 1.72MB/s]\n",
      "products_train.csv:   5%|4         | 29.4M/589M [00:16<05:26, 1.71MB/s]\n",
      "products_train.csv:   5%|5         | 30.4M/589M [00:17<05:26, 1.71MB/s]\n",
      "products_train.csv:   5%|5         | 31.5M/589M [00:17<05:26, 1.70MB/s]\n",
      "products_train.csv:   6%|5         | 32.5M/589M [00:18<05:26, 1.70MB/s]\n",
      "products_train.csv:   6%|5         | 33.6M/589M [00:19<05:25, 1.70MB/s]\n",
      "products_train.csv:   6%|5         | 34.6M/589M [00:19<05:25, 1.70MB/s]\n",
      "products_train.csv:   6%|6         | 35.7M/589M [00:20<05:41, 1.62MB/s]\n",
      "products_train.csv:   6%|6         | 36.7M/589M [00:21<06:08, 1.50MB/s]\n",
      "products_train.csv:   6%|6         | 37.7M/589M [00:22<07:13, 1.27MB/s]\n",
      "products_train.csv:   7%|6         | 38.8M/589M [00:23<06:57, 1.32MB/s]\n",
      "products_train.csv:   7%|6         | 39.8M/589M [00:23<05:58, 1.53MB/s]\n",
      "products_train.csv:   7%|6         | 40.9M/589M [00:24<05:35, 1.63MB/s]\n",
      "products_train.csv:   7%|7         | 41.9M/589M [00:24<05:24, 1.68MB/s]\n",
      "products_train.csv:   7%|7         | 43.0M/589M [00:25<05:23, 1.69MB/s]\n",
      "products_train.csv:   7%|7         | 44.0M/589M [00:25<05:22, 1.69MB/s]\n",
      "products_train.csv:   8%|7         | 45.1M/589M [00:26<05:13, 1.74MB/s]\n",
      "products_train.csv:   8%|7         | 46.1M/589M [00:27<05:16, 1.71MB/s]\n",
      "products_train.csv:   8%|8         | 47.2M/589M [00:27<05:07, 1.76MB/s]\n",
      "products_train.csv:   8%|8         | 48.2M/589M [00:28<04:59, 1.80MB/s]\n",
      "products_train.csv:   8%|8         | 49.3M/589M [00:28<04:52, 1.85MB/s]\n",
      "products_train.csv:   9%|8         | 50.3M/589M [00:29<04:43, 1.90MB/s]\n",
      "products_train.csv:   9%|8         | 51.4M/589M [00:29<04:29, 2.00MB/s]\n",
      "products_train.csv:   9%|8         | 52.4M/589M [00:30<04:20, 2.06MB/s]\n",
      "products_train.csv:   9%|9         | 53.5M/589M [00:30<04:11, 2.13MB/s]\n",
      "products_train.csv:   9%|9         | 54.5M/589M [00:31<03:52, 2.30MB/s]\n",
      "products_train.csv:   9%|9         | 55.6M/589M [00:31<03:33, 2.50MB/s]\n",
      "products_train.csv:  10%|9         | 56.6M/589M [00:31<03:22, 2.63MB/s]\n",
      "products_train.csv:  10%|9         | 57.7M/589M [00:32<03:22, 2.62MB/s]\n",
      "products_train.csv:  10%|9         | 58.7M/589M [00:32<03:10, 2.78MB/s]\n",
      "products_train.csv:  10%|#         | 59.8M/589M [00:33<03:50, 2.29MB/s]\n",
      "products_train.csv:  10%|#         | 60.8M/589M [00:33<04:13, 2.08MB/s]\n",
      "products_train.csv:  11%|#         | 61.9M/589M [00:33<03:29, 2.52MB/s]\n",
      "products_train.csv:  11%|#         | 62.9M/589M [00:34<03:13, 2.72MB/s]\n",
      "products_train.csv:  11%|#         | 64.0M/589M [00:34<03:01, 2.90MB/s]\n",
      "products_train.csv:  11%|#1        | 65.0M/589M [00:35<03:12, 2.71MB/s]\n",
      "products_train.csv:  11%|#1        | 66.1M/589M [00:35<03:43, 2.34MB/s]\n",
      "products_train.csv:  11%|#1        | 67.1M/589M [00:36<04:15, 2.04MB/s]\n",
      "products_train.csv:  12%|#1        | 68.2M/589M [00:36<03:20, 2.59MB/s]\n",
      "products_train.csv:  12%|#1        | 70.3M/589M [00:36<02:27, 3.52MB/s]\n",
      "products_train.csv:  12%|#2        | 71.3M/589M [00:37<02:32, 3.39MB/s]\n",
      "products_train.csv:  12%|#2        | 72.4M/589M [00:37<02:39, 3.23MB/s]\n",
      "products_train.csv:  12%|#2        | 73.4M/589M [00:37<02:40, 3.20MB/s]\n",
      "products_train.csv:  13%|#2        | 74.4M/589M [00:38<02:39, 3.21MB/s]\n",
      "products_train.csv:  13%|#2        | 75.5M/589M [00:40<06:38, 1.29MB/s]\n",
      "products_train.csv:  13%|#3        | 76.5M/589M [00:40<05:18, 1.61MB/s]\n",
      "products_train.csv:  13%|#3        | 78.6M/589M [00:40<03:46, 2.25MB/s]\n",
      "products_train.csv:  14%|#3        | 79.7M/589M [00:41<03:20, 2.54MB/s]\n",
      "products_train.csv:  14%|#3        | 80.7M/589M [00:41<03:07, 2.71MB/s]\n",
      "products_train.csv:  14%|#3        | 81.8M/589M [00:41<02:59, 2.82MB/s]\n",
      "products_train.csv:  14%|#4        | 82.8M/589M [00:42<02:49, 2.99MB/s]\n",
      "products_train.csv:  14%|#4        | 83.9M/589M [00:42<02:48, 3.00MB/s]\n",
      "products_train.csv:  14%|#4        | 84.9M/589M [00:42<02:42, 3.10MB/s]\n",
      "products_train.csv:  15%|#4        | 86.0M/589M [00:43<02:40, 3.14MB/s]\n",
      "products_train.csv:  15%|#4        | 87.0M/589M [00:43<02:37, 3.18MB/s]\n",
      "products_train.csv:  15%|#4        | 88.1M/589M [00:43<02:41, 3.10MB/s]\n",
      "products_train.csv:  15%|#5        | 89.1M/589M [00:44<04:18, 1.93MB/s]\n",
      "products_train.csv:  15%|#5        | 90.2M/589M [00:44<03:20, 2.48MB/s]\n",
      "products_train.csv:  16%|#5        | 91.2M/589M [00:45<02:45, 3.00MB/s]\n",
      "products_train.csv:  16%|#5        | 92.3M/589M [00:45<02:47, 2.96MB/s]\n",
      "products_train.csv:  16%|#5        | 93.3M/589M [00:45<02:46, 2.98MB/s]\n",
      "products_train.csv:  16%|#6        | 94.4M/589M [00:46<02:38, 3.12MB/s]\n",
      "products_train.csv:  16%|#6        | 95.4M/589M [00:46<02:44, 2.99MB/s]\n",
      "products_train.csv:  16%|#6        | 96.5M/589M [00:47<04:18, 1.90MB/s]\n",
      "products_train.csv:  17%|#6        | 97.5M/589M [00:47<03:56, 2.08MB/s]\n",
      "products_train.csv:  17%|#6        | 98.6M/589M [00:49<07:31, 1.09MB/s]\n",
      "products_train.csv:  17%|#6        | 99.6M/589M [00:50<06:02, 1.35MB/s]\n",
      "products_train.csv:  17%|#7        | 101M/589M [00:51<06:41, 1.22MB/s] \n",
      "products_train.csv:  17%|#7        | 102M/589M [00:51<05:43, 1.42MB/s]\n",
      "products_train.csv:  17%|#7        | 103M/589M [00:52<04:54, 1.65MB/s]\n",
      "products_train.csv:  18%|#7        | 104M/589M [00:52<04:24, 1.83MB/s]\n",
      "products_train.csv:  18%|#7        | 105M/589M [00:52<03:51, 2.09MB/s]\n",
      "products_train.csv:  18%|#7        | 106M/589M [00:53<03:26, 2.34MB/s]\n",
      "products_train.csv:  18%|#8        | 107M/589M [00:53<03:16, 2.46MB/s]\n",
      "products_train.csv:  18%|#8        | 108M/589M [00:54<03:17, 2.43MB/s]\n",
      "products_train.csv:  19%|#8        | 109M/589M [00:55<04:35, 1.74MB/s]\n",
      "products_train.csv:  19%|#8        | 110M/589M [00:55<03:37, 2.20MB/s]\n",
      "products_train.csv:  19%|#8        | 111M/589M [00:56<05:00, 1.59MB/s]\n",
      "products_train.csv:  19%|#9        | 112M/589M [00:56<04:09, 1.91MB/s]\n",
      "products_train.csv:  19%|#9        | 113M/589M [00:56<03:13, 2.46MB/s]\n",
      "products_train.csv:  19%|#9        | 114M/589M [00:57<02:58, 2.66MB/s]\n",
      "products_train.csv:  20%|#9        | 115M/589M [00:57<02:51, 2.77MB/s]\n",
      "products_train.csv:  20%|#9        | 116M/589M [00:57<02:48, 2.80MB/s]\n",
      "products_train.csv:  20%|#9        | 117M/589M [00:58<02:44, 2.87MB/s]\n",
      "products_train.csv:  20%|##        | 118M/589M [00:58<02:49, 2.78MB/s]\n",
      "products_train.csv:  20%|##        | 120M/589M [00:59<04:14, 1.84MB/s]\n",
      "products_train.csv:  20%|##        | 121M/589M [00:59<03:30, 2.22MB/s]\n",
      "products_train.csv:  21%|##        | 122M/589M [01:00<02:58, 2.61MB/s]\n",
      "products_train.csv:  21%|##        | 123M/589M [01:00<02:54, 2.67MB/s]\n",
      "products_train.csv:  21%|##1       | 124M/589M [01:00<02:48, 2.76MB/s]\n"
     ]
    }
   ],
   "source": [
    "!aicrowd dataset download --challenge task-1-next-product-recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f9533",
   "metadata": {},
   "source": [
    "## Setup data and task information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "b32f5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e6a9e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'train_data'\n",
    "test_data_dir = 'test_data'\n",
    "task = 'task1'\n",
    "PREDS_PER_SESSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4e71327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache loading of data for multiple calls\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def read_product_data():\n",
    "    return pd.read_csv(os.path.join(train_data_dir, 'products_train.csv'))\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def read_train_data():\n",
    "    return pd.read_csv(os.path.join(train_data_dir, 'sessions_train.csv'))\n",
    "\n",
    "@lru_cache(maxsize=3)\n",
    "def read_test_data(task):\n",
    "    return pd.read_csv(os.path.join(test_data_dir, f'sessions_test_{task}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333b75e",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "The Multilingual Shopping Session Dataset is a collection of <b>anonymized customer sessions</b> containing products from six different locales, namely English, German, Japanese, French, Italian, and Spanish. It consists of two main components: user sessions and product attributes. User sessions are a list of products that a user has engaged with in chronological order, while product attributes include various details like product title, price in local currency, brand, color, and description.\n",
    "****\n",
    "### Each product has its associated information:\n",
    "- <b>locale</b>: The locale code of the product.\n",
    "- <b>id</b>: A unique id for the product.\n",
    "- <b>title</b>: Title of the item (e.g., \"Japanese Aesthetic Sakura Flowers Vaporwave Soft Grunge Gift T-Shirt\")\n",
    "- <b>price</b>: Price of the item in local currency (e.g., 24.99)\n",
    "- <b>brand</b>: item brand name (e.g., “Japanese Aesthetic Flowers & Vaporwave Clothing”)\n",
    "- <b>color</b>: color of the item (e.g., “Black”)\n",
    "- <b>size</b>: size of the item (e.g., “xxl”)\n",
    "- <b>model</b>: model of the item (e.g., “iphone 13”)\n",
    "- <b>material</b>: material of the item (e.g., “cotton”)\n",
    "- <b>author</b>: author of the item (e.g., “J. K. Rowling”)\n",
    "- <b>desc</b>: description about a item’s key features and benefits called out via bullet points (e.g., “Solid colors: 100% Cotton; Heather Grey: 90% Cotton, 10% Polyester; All Other Heathers …”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb17734",
   "metadata": {},
   "source": [
    "## EDA 📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6023e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_locale_data(locale, task):\n",
    "    products = read_product_data().query(f'locale == \"{locale}\"')\n",
    "    sess_train = read_train_data().query(f'locale == \"{locale}\"')\n",
    "    sess_test = read_test_data(task).query(f'locale == \"{locale}\"')\n",
    "    return products, sess_train, sess_test\n",
    "\n",
    "def show_locale_info(locale, task):\n",
    "    products, sess_train, sess_test = read_locale_data(locale, task)\n",
    "\n",
    "    train_l = sess_train['prev_items'].apply(lambda sess: len(sess))\n",
    "    test_l = sess_test['prev_items'].apply(lambda sess: len(sess))\n",
    "\n",
    "    print(f\"Locale: {locale} \\n\"\n",
    "          f\"Number of products: {products['id'].nunique()} \\n\"\n",
    "          f\"Number of train sessions: {len(sess_train)} \\n\"\n",
    "          f\"Train session lengths - \"\n",
    "          f\"Mean: {train_l.mean():.2f} | Median {train_l.median():.2f} | \"\n",
    "          f\"Min: {train_l.min():.2f} | Max {train_l.max():.2f} \\n\"\n",
    "          f\"Number of test sessions: {len(sess_test)}\"\n",
    "        )\n",
    "    if len(sess_test) > 0:\n",
    "        print(\n",
    "             f\"Test session lengths - \"\n",
    "            f\"Mean: {test_l.mean():.2f} | Median {test_l.median():.2f} | \"\n",
    "            f\"Min: {test_l.min():.2f} | Max {test_l.max():.2f} \\n\"\n",
    "        )\n",
    "    print(\"======================================================================== \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e204e4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locale: DE \n",
      "Number of products: 518327 \n",
      "Number of train sessions: 1111416 \n",
      "Train session lengths - Mean: 57.89 | Median 40.00 | Min: 27.00 | Max 2060.00 \n",
      "Number of test sessions: 104568\n",
      "Test session lengths - Mean: 56.91 | Median 40.00 | Min: 27.00 | Max 1043.00 \n",
      "\n",
      "======================================================================== \n",
      "\n",
      "Locale: JP \n",
      "Number of products: 395009 \n",
      "Number of train sessions: 979119 \n",
      "Train session lengths - Mean: 59.61 | Median 40.00 | Min: 27.00 | Max 6257.00 \n",
      "Number of test sessions: 96467\n",
      "Test session lengths - Mean: 59.84 | Median 40.00 | Min: 27.00 | Max 1466.00 \n",
      "\n",
      "======================================================================== \n",
      "\n",
      "Locale: UK \n",
      "Number of products: 500180 \n",
      "Number of train sessions: 1182181 \n",
      "Train session lengths - Mean: 54.85 | Median 40.00 | Min: 27.00 | Max 2654.00 \n",
      "Number of test sessions: 115937\n",
      "Test session lengths - Mean: 53.25 | Median 40.00 | Min: 27.00 | Max 753.00 \n",
      "\n",
      "======================================================================== \n",
      "\n",
      "Locale: ES \n",
      "Number of products: 42503 \n",
      "Number of train sessions: 89047 \n",
      "Train session lengths - Mean: 48.82 | Median 40.00 | Min: 27.00 | Max 792.00 \n",
      "Number of test sessions: 0\n",
      "======================================================================== \n",
      "\n",
      "Locale: FR \n",
      "Number of products: 44577 \n",
      "Number of train sessions: 117561 \n",
      "Train session lengths - Mean: 47.25 | Median 40.00 | Min: 27.00 | Max 687.00 \n",
      "Number of test sessions: 0\n",
      "======================================================================== \n",
      "\n",
      "Locale: IT \n",
      "Number of products: 50461 \n",
      "Number of train sessions: 126925 \n",
      "Train session lengths - Mean: 48.80 | Median 40.00 | Min: 27.00 | Max 621.00 \n",
      "Number of test sessions: 0\n",
      "======================================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = read_product_data()\n",
    "locale_names = products['locale'].unique()\n",
    "for locale in locale_names:\n",
    "    show_locale_info(locale, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbb1fa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>locale</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>model</th>\n",
       "      <th>material</th>\n",
       "      <th>author</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1427524</th>\n",
       "      <td>B08QS5XR31</td>\n",
       "      <td>ES</td>\n",
       "      <td>Auxmir Espejo de Maquillaje con Aumento 1X / 7...</td>\n",
       "      <td>16.49</td>\n",
       "      <td>Auxmir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Espejo de mesa 1X/7X</td>\n",
       "      <td>‎</td>\n",
       "      <td>Vidrio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ALTURA PERFECTA &amp; MÁS CÓMODO]: El espejo maqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859608</th>\n",
       "      <td>B0BG3NV6TM</td>\n",
       "      <td>JP</td>\n",
       "      <td>【眼科医推薦】 C_himawari シャンプーハット バスグッズ お風呂 赤ちゃん 大人 ...</td>\n",
       "      <td>1450.00</td>\n",
       "      <td>C_himawari</td>\n",
       "      <td>ホワイト</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【21段階調節可能】21段階の調節が可能で大人から子供まで使用いただけます。頭囲40～61ｃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755574</th>\n",
       "      <td>B000JCEPVE</td>\n",
       "      <td>JP</td>\n",
       "      <td>エレコム インクジェット用紙 スーパーファイン マット紙 A4 200枚 用 薄手 片面 日...</td>\n",
       "      <td>788.00</td>\n",
       "      <td>エレコム</td>\n",
       "      <td>薄手</td>\n",
       "      <td>A4 200枚</td>\n",
       "      <td>EJK-SUA4200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>インクジェットプリンタ用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80133</th>\n",
       "      <td>B09YY22MDN</td>\n",
       "      <td>DE</td>\n",
       "      <td>16XL Multipack Druckerpatronen Kompatibel für ...</td>\n",
       "      <td>16.58</td>\n",
       "      <td>KEENKLE</td>\n",
       "      <td>3 Cyan, 3 Magenta, 3 Gelb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16 XL 16XL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16XL 16 Multipack Druckerpatronen Kompatible f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473986</th>\n",
       "      <td>B01GEOU5IC</td>\n",
       "      <td>FR</td>\n",
       "      <td>Lot de 2 paquets Jelly Belly Bean Boozled Harr...</td>\n",
       "      <td>14.99</td>\n",
       "      <td>Jelly Belly</td>\n",
       "      <td>multicolore</td>\n",
       "      <td>54 g (Lot de 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id locale                                              title  \\\n",
       "1427524  B08QS5XR31     ES  Auxmir Espejo de Maquillaje con Aumento 1X / 7...   \n",
       "859608   B0BG3NV6TM     JP  【眼科医推薦】 C_himawari シャンプーハット バスグッズ お風呂 赤ちゃん 大人 ...   \n",
       "755574   B000JCEPVE     JP  エレコム インクジェット用紙 スーパーファイン マット紙 A4 200枚 用 薄手 片面 日...   \n",
       "80133    B09YY22MDN     DE  16XL Multipack Druckerpatronen Kompatibel für ...   \n",
       "1473986  B01GEOU5IC     FR  Lot de 2 paquets Jelly Belly Bean Boozled Harr...   \n",
       "\n",
       "           price        brand                      color  \\\n",
       "1427524    16.49       Auxmir                        NaN   \n",
       "859608   1450.00   C_himawari                       ホワイト   \n",
       "755574    788.00         エレコム                         薄手   \n",
       "80133      16.58      KEENKLE  3 Cyan, 3 Magenta, 3 Gelb   \n",
       "1473986    14.99  Jelly Belly                multicolore   \n",
       "\n",
       "                         size        model material author  \\\n",
       "1427524  Espejo de mesa 1X/7X            ‎   Vidrio    NaN   \n",
       "859608                    NaN          NaN      NaN    NaN   \n",
       "755574                A4 200枚  EJK-SUA4200      NaN    NaN   \n",
       "80133                     NaN   16 XL 16XL      NaN    NaN   \n",
       "1473986       54 g (Lot de 2)          NaN      NaN    NaN   \n",
       "\n",
       "                                                      desc  \n",
       "1427524  [ALTURA PERFECTA & MÁS CÓMODO]: El espejo maqu...  \n",
       "859608   【21段階調節可能】21段階の調節が可能で大人から子供まで使用いただけます。頭囲40～61ｃ...  \n",
       "755574                                        インクジェットプリンタ用  \n",
       "80133    16XL 16 Multipack Druckerpatronen Kompatible f...  \n",
       "1473986                                                NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c178d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2069988</th>\n",
       "      <td>['B09L3ZSC41' 'B09L4GHYN4' 'B09L3ZSC41']</td>\n",
       "      <td>B09L44ZRH2</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488076</th>\n",
       "      <td>['B01N0M3H1L' 'B009GXMFU0' 'B009GXMFU0']</td>\n",
       "      <td>B009GXMEV0</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793020</th>\n",
       "      <td>['B09ZXX7KQJ' 'B09THFQVJ7']</td>\n",
       "      <td>B09ZNQW3TL</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743944</th>\n",
       "      <td>['B086WMC9VW' 'B086WMC9VW' 'B086WMC9VW' 'B08QN...</td>\n",
       "      <td>B09DKZ9P72</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650458</th>\n",
       "      <td>['B09RWLYH3H' 'B07H8DPJKM']</td>\n",
       "      <td>B09CTHJX9T</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prev_items   next_item locale\n",
       "2069988           ['B09L3ZSC41' 'B09L4GHYN4' 'B09L3ZSC41']  B09L44ZRH2     JP\n",
       "1488076           ['B01N0M3H1L' 'B009GXMFU0' 'B009GXMFU0']  B009GXMEV0     JP\n",
       "1793020                        ['B09ZXX7KQJ' 'B09THFQVJ7']  B09ZNQW3TL     JP\n",
       "743944   ['B086WMC9VW' 'B086WMC9VW' 'B086WMC9VW' 'B08QN...  B09DKZ9P72     DE\n",
       "1650458                        ['B09RWLYH3H' 'B07H8DPJKM']  B09CTHJX9T     JP"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions = read_train_data()\n",
    "train_sessions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e7ac501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105574</th>\n",
       "      <td>['B09WN27199' 'B0B7LG5QFL' 'B09GVW323Q' 'B09D4...</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196989</th>\n",
       "      <td>['B09RQF9J65' 'B09CT4788X' 'B09RQD8V4C' 'B09CT...</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315813</th>\n",
       "      <td>['1471137155' '1542025605']</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242925</th>\n",
       "      <td>['B0B62G97YC' 'B0B62G97YC' 'B09YXQB57T']</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288675</th>\n",
       "      <td>['B00K80OYXQ' 'B07RXBVNBQ' 'B07RXBVJXY']</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prev_items locale\n",
       "105574  ['B09WN27199' 'B0B7LG5QFL' 'B09GVW323Q' 'B09D4...     JP\n",
       "196989  ['B09RQF9J65' 'B09CT4788X' 'B09RQD8V4C' 'B09CT...     JP\n",
       "315813                        ['1471137155' '1542025605']     UK\n",
       "242925           ['B0B62G97YC' 'B0B62G97YC' 'B09YXQB57T']     UK\n",
       "288675           ['B00K80OYXQ' 'B07RXBVNBQ' 'B07RXBVJXY']     UK"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions = read_test_data(task)\n",
    "test_sessions.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af87c4",
   "metadata": {},
   "source": [
    "## Generate Submission 🏋️‍♀️\n",
    "\n",
    "Submission format:\n",
    "<ol>\n",
    "    <li>The submission should be a parquet file with the sessions from all the locales.</li>\n",
    "    <li>Predicted products ids per locale should only be a valid product id of that locale.</li>\n",
    "    <li>Predictions should be added in new column named \"next_item_prediction\".</li>\n",
    "    <li>Predictions should be a list of string id values</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "48c51a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_locale = 'IT'\n",
    "locale_product, locale_train_sess, locale_test_sess = read_locale_data(current_locale, task)\n",
    "locale_product.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "fe593067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(x, x_index, products_df):\n",
    "    hor_indices = np.where(products_df['id'].isin(x))[0].reshape(-1, 1)\n",
    "    ver_indices = np.ones((len(hor_indices), 1))*x_index\n",
    "    return np.hstack((ver_indices, hor_indices)).astype('int')\n",
    "\n",
    "def generate_batch_sparse_tensor(products_df, locale_session_data, batch_size=10000):\n",
    "    batch_data = locale_session_data.sample(n=batch_size)\n",
    "    \n",
    "    items_viewed = batch_data['prev_items'].to_numpy()\n",
    "    max_length = batch_data['prev_items'].apply(lambda sess: len(sess)).max()\n",
    "    items_viewed = items_viewed.astype(f'<U{max_length}')\n",
    "    stripped_items = np.char.strip(np.char.strip(items_viewed, r'[]'))\n",
    "    replaced_items = np.char.replace(np.char.replace(stripped_items, '\\'', ''), '\\n', '')\n",
    "    splitted_items = np.char.split(replaced_items)\n",
    "    indices = list(map(lambda x: get_indices(x[1], x[0], products_df), enumerate(splitted_items)))\n",
    "    indices = np.concatenate(indices)\n",
    "    values = list(np.ones(indices.shape[0]))\n",
    "    return tf.SparseTensor(indices=indices,\n",
    "                          values=values, \n",
    "                          dense_shape=[batch_data.shape[0], len(locale_product)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "dc0d9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mean_squared_error(sparse_products, predictions):\n",
    "    loss = tf.losses.mean_squared_error(sparse_products.values, predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "0c791632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon standard identification number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_embeddings: shape (no_of_user_sessions, d)\n",
    "# product_embeddings: shape (no_of_product_ids, d)\n",
    "# d is the number of features encoded in the embedding\n",
    "# predictions (model): user_embeddings.(product_embeddings^T)\n",
    "# loss = sum((Aij - <Ui.Vj>)^2) for i, j belongs to all observed states.\n",
    "# minimize loss using stochastic gradient descent.\n",
    "# U = U - (lr)(dL/dU)\n",
    "# V = V - (lr)(dL/DV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "5347d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(tf.Module):\n",
    "    def __init__(self, embedding_nfeatures=1):\n",
    "        super(CFModel, self).__init__()\n",
    "        self.embedding_nfeatures = embedding_nfeatures\n",
    "        self.build_embeddings = True\n",
    "        self.loss_history = []\n",
    "        self.iterations = []\n",
    "    \n",
    "    @property\n",
    "    def get_embeddings(self):\n",
    "        return {'user': self.user_embeddings, 'product': self.product_embeddings}\n",
    "    \n",
    "    def build(self, user_n, product_n):\n",
    "        normal_init = tf.random_normal_initializer()\n",
    "        \n",
    "        self.user_embeddings = tf.Variable(\n",
    "            initial_value=normal_init(shape=(user_n, self.embedding_nfeatures), dtype='float32'),\n",
    "            trainable=True, name='user_embeddings')\n",
    "        \n",
    "        self.product_embeddings = tf.Variable(\n",
    "            initial_value=normal_init(shape=(product_n, self.embedding_nfeatures), dtype='float32'),\n",
    "            trainable=True, name='product_embeddings')\n",
    "        \n",
    "    def __call__(self, session_tensor):\n",
    "        user_n = session_tensor.dense_shape[0]\n",
    "        product_n = session_tensor.dense_shape[1]\n",
    "        if self.build_embeddings:\n",
    "            self.build(user_n, product_n)\n",
    "            self.build_embeddings = False\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = CFModel.predict(session_tensor, self.user_embeddings, self.product_embeddings)\n",
    "            loss = self.loss_func(session_tensor, prediction)\n",
    "            \n",
    "        embeddings_vars = {'user':self.user_embeddings, 'product':self.product_embeddings}\n",
    "        grads = tape.gradient(loss, embeddings_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads.values(), embeddings_vars.values()))\n",
    "        \n",
    "        self.loss_history.append(loss.numpy())\n",
    "        \n",
    "    \n",
    "    def fit(self, session_tensor, loss_func, iterations=10, learning_rate=1.0, optimizer=tf.keras.optimizers.SGD, debug=False):\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer(learning_rate=learning_rate)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            self.__call__(session_tensor)\n",
    "            self.iterations.append(i)\n",
    "            if debug:\n",
    "                print(f'Iteration: {i} | Loss: {self.loss_history[-1]}')\n",
    "    \n",
    "    \n",
    "    def plot_loss(self):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "        ax.plot(self.iterations, self.loss_history, lw=2, marker='o')\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Model training loss history')\n",
    "        \n",
    "    \n",
    "    def predict(session_tensor, user_embeddings, product_embeddings):\n",
    "        if (user_embeddings.shape[-1] != product_embeddings.shape[-1]):\n",
    "            raise Exception(\n",
    "                f'user embeddings having features {user_embeddings.shape[-1]} and product embeddings having features {product_embeddings.shape[-1]} are incompatible.')\n",
    "        \n",
    "        return tf.reduce_sum(\n",
    "            tf.gather(user_embeddings, session_tensor.indices[:, 0]) *\n",
    "            tf.gather(product_embeddings, session_tensor.indices[:, 1]),\n",
    "            axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "d18ecaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_sparse_tensor = generate_batch_sparse_tensor(locale_product, locale_train_sess, batch_size=10000)\n",
    "loss_func = sparse_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "68e83c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = CFModel(embedding_nfeatures=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "75f7d3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Loss: 0.20821461081504822\n",
      "Iteration: 1 | Loss: 0.20222796499729156\n",
      "Iteration: 2 | Loss: 0.19638267159461975\n",
      "Iteration: 3 | Loss: 0.1906767189502716\n",
      "Iteration: 4 | Loss: 0.18510808050632477\n",
      "Iteration: 5 | Loss: 0.17967469990253448\n",
      "Iteration: 6 | Loss: 0.1743745356798172\n",
      "Iteration: 7 | Loss: 0.1692054569721222\n",
      "Iteration: 8 | Loss: 0.16416533291339874\n",
      "Iteration: 9 | Loss: 0.15925200283527374\n",
      "Iteration: 10 | Loss: 0.15446339547634125\n",
      "Iteration: 11 | Loss: 0.14979729056358337\n",
      "Iteration: 12 | Loss: 0.14525152742862701\n",
      "Iteration: 13 | Loss: 0.14082396030426025\n",
      "Iteration: 14 | Loss: 0.1365123987197876\n",
      "Iteration: 15 | Loss: 0.13231465220451355\n",
      "Iteration: 16 | Loss: 0.1282285898923874\n",
      "Iteration: 17 | Loss: 0.12425200641155243\n",
      "Iteration: 18 | Loss: 0.12038274854421616\n",
      "Iteration: 19 | Loss: 0.11661864817142487\n",
      "Iteration: 20 | Loss: 0.11295758932828903\n",
      "Iteration: 21 | Loss: 0.10939738899469376\n",
      "Iteration: 22 | Loss: 0.1059359461069107\n",
      "Iteration: 23 | Loss: 0.10257114470005035\n",
      "Iteration: 24 | Loss: 0.09930089116096497\n",
      "Iteration: 25 | Loss: 0.0961230918765068\n",
      "Iteration: 26 | Loss: 0.09303571283817291\n",
      "Iteration: 27 | Loss: 0.09003667533397675\n",
      "Iteration: 28 | Loss: 0.08712397515773773\n",
      "Iteration: 29 | Loss: 0.0842956081032753\n",
      "Iteration: 30 | Loss: 0.08154960721731186\n",
      "Iteration: 31 | Loss: 0.07888402789831161\n",
      "Iteration: 32 | Loss: 0.07629692554473877\n",
      "Iteration: 33 | Loss: 0.0737864226102829\n",
      "Iteration: 34 | Loss: 0.07135064899921417\n",
      "Iteration: 35 | Loss: 0.06898775696754456\n",
      "Iteration: 36 | Loss: 0.06669594347476959\n",
      "Iteration: 37 | Loss: 0.06447342038154602\n",
      "Iteration: 38 | Loss: 0.06231844425201416\n",
      "Iteration: 39 | Loss: 0.06022929027676582\n",
      "Iteration: 40 | Loss: 0.05820427089929581\n",
      "Iteration: 41 | Loss: 0.056241728365421295\n",
      "Iteration: 42 | Loss: 0.05434003472328186\n",
      "Iteration: 43 | Loss: 0.05249760299921036\n",
      "Iteration: 44 | Loss: 0.050712864845991135\n",
      "Iteration: 45 | Loss: 0.048984285444021225\n",
      "Iteration: 46 | Loss: 0.04731036350131035\n",
      "Iteration: 47 | Loss: 0.04568962752819061\n",
      "Iteration: 48 | Loss: 0.04412063583731651\n",
      "Iteration: 49 | Loss: 0.04260198399424553\n",
      "Iteration: 50 | Loss: 0.04113227501511574\n",
      "Iteration: 51 | Loss: 0.039710164070129395\n",
      "Iteration: 52 | Loss: 0.038334332406520844\n",
      "Iteration: 53 | Loss: 0.03700347989797592\n",
      "Iteration: 54 | Loss: 0.03571633622050285\n",
      "Iteration: 55 | Loss: 0.03447166085243225\n",
      "Iteration: 56 | Loss: 0.03326824679970741\n",
      "Iteration: 57 | Loss: 0.03210489824414253\n",
      "Iteration: 58 | Loss: 0.030980456620454788\n",
      "Iteration: 59 | Loss: 0.0298937875777483\n",
      "Iteration: 60 | Loss: 0.028843777254223824\n",
      "Iteration: 61 | Loss: 0.02782933972775936\n",
      "Iteration: 62 | Loss: 0.02684941329061985\n",
      "Iteration: 63 | Loss: 0.025902962312102318\n",
      "Iteration: 64 | Loss: 0.024988971650600433\n",
      "Iteration: 65 | Loss: 0.024106450378894806\n",
      "Iteration: 66 | Loss: 0.023254428058862686\n",
      "Iteration: 67 | Loss: 0.022431958466768265\n",
      "Iteration: 68 | Loss: 0.021638117730617523\n",
      "Iteration: 69 | Loss: 0.02087200991809368\n",
      "Iteration: 70 | Loss: 0.020132748410105705\n",
      "Iteration: 71 | Loss: 0.019419478252530098\n",
      "Iteration: 72 | Loss: 0.018731357529759407\n",
      "Iteration: 73 | Loss: 0.018067575991153717\n",
      "Iteration: 74 | Loss: 0.017427336424589157\n",
      "Iteration: 75 | Loss: 0.01680986024439335\n",
      "Iteration: 76 | Loss: 0.01621439680457115\n",
      "Iteration: 77 | Loss: 0.015640204772353172\n",
      "Iteration: 78 | Loss: 0.015086573548614979\n",
      "Iteration: 79 | Loss: 0.014552805572748184\n",
      "Iteration: 80 | Loss: 0.014038221910595894\n",
      "Iteration: 81 | Loss: 0.013542166911065578\n",
      "Iteration: 82 | Loss: 0.013064000755548477\n",
      "Iteration: 83 | Loss: 0.012603101320564747\n",
      "Iteration: 84 | Loss: 0.012158866040408611\n",
      "Iteration: 85 | Loss: 0.011730710975825787\n",
      "Iteration: 86 | Loss: 0.011318068951368332\n",
      "Iteration: 87 | Loss: 0.010920388624072075\n",
      "Iteration: 88 | Loss: 0.01053713634610176\n",
      "Iteration: 89 | Loss: 0.010167795233428478\n",
      "Iteration: 90 | Loss: 0.009811867959797382\n",
      "Iteration: 91 | Loss: 0.00946886744350195\n",
      "Iteration: 92 | Loss: 0.009138328023254871\n",
      "Iteration: 93 | Loss: 0.008819794282317162\n",
      "Iteration: 94 | Loss: 0.008512829430401325\n",
      "Iteration: 95 | Loss: 0.008217008784413338\n",
      "Iteration: 96 | Loss: 0.007931924425065517\n",
      "Iteration: 97 | Loss: 0.007657181937247515\n",
      "Iteration: 98 | Loss: 0.007392397616058588\n",
      "Iteration: 99 | Loss: 0.0071372054517269135\n",
      "Iteration: 100 | Loss: 0.006891249679028988\n",
      "Iteration: 101 | Loss: 0.0066541885025799274\n",
      "Iteration: 102 | Loss: 0.006425690371543169\n",
      "Iteration: 103 | Loss: 0.006205437704920769\n",
      "Iteration: 104 | Loss: 0.0059931231662631035\n",
      "Iteration: 105 | Loss: 0.00578845152631402\n",
      "Iteration: 106 | Loss: 0.005591138266026974\n",
      "Iteration: 107 | Loss: 0.005400908645242453\n",
      "Iteration: 108 | Loss: 0.005217498634010553\n",
      "Iteration: 109 | Loss: 0.005040654446929693\n",
      "Iteration: 110 | Loss: 0.00487013254314661\n",
      "Iteration: 111 | Loss: 0.0047056954354047775\n",
      "Iteration: 112 | Loss: 0.004547117277979851\n",
      "Iteration: 113 | Loss: 0.004394181538373232\n",
      "Iteration: 114 | Loss: 0.004246677737683058\n",
      "Iteration: 115 | Loss: 0.004104404244571924\n",
      "Iteration: 116 | Loss: 0.0039671678096055984\n",
      "Iteration: 117 | Loss: 0.00383478170260787\n",
      "Iteration: 118 | Loss: 0.0037070666439831257\n",
      "Iteration: 119 | Loss: 0.003583851270377636\n",
      "Iteration: 120 | Loss: 0.0034649698063731194\n",
      "Iteration: 121 | Loss: 0.003350262762978673\n",
      "Iteration: 122 | Loss: 0.0032395776361227036\n",
      "Iteration: 123 | Loss: 0.003132766578346491\n",
      "Iteration: 124 | Loss: 0.003029689658433199\n",
      "Iteration: 125 | Loss: 0.0029302099719643593\n",
      "Iteration: 126 | Loss: 0.0028341971337795258\n",
      "Iteration: 127 | Loss: 0.0027415261138230562\n",
      "Iteration: 128 | Loss: 0.0026520758401602507\n",
      "Iteration: 129 | Loss: 0.0025657303631305695\n",
      "Iteration: 130 | Loss: 0.002482378389686346\n",
      "Iteration: 131 | Loss: 0.0024019123520702124\n",
      "Iteration: 132 | Loss: 0.002324229571968317\n",
      "Iteration: 133 | Loss: 0.002249230397865176\n",
      "Iteration: 134 | Loss: 0.002176819834858179\n",
      "Iteration: 135 | Loss: 0.002106906147673726\n",
      "Iteration: 136 | Loss: 0.0020394008606672287\n",
      "Iteration: 137 | Loss: 0.001974219223484397\n",
      "Iteration: 138 | Loss: 0.00191127962898463\n",
      "Iteration: 139 | Loss: 0.0018505030311644077\n",
      "Iteration: 140 | Loss: 0.001791813992895186\n",
      "Iteration: 141 | Loss: 0.0017351395217701793\n",
      "Iteration: 142 | Loss: 0.0016804095357656479\n",
      "Iteration: 143 | Loss: 0.0016275562811642885\n",
      "Iteration: 144 | Loss: 0.0015765149146318436\n",
      "Iteration: 145 | Loss: 0.0015272223390638828\n",
      "Iteration: 146 | Loss: 0.0014796181349083781\n",
      "Iteration: 147 | Loss: 0.001433644094504416\n",
      "Iteration: 148 | Loss: 0.0013892441056668758\n",
      "Iteration: 149 | Loss: 0.0013463639188557863\n",
      "Iteration: 150 | Loss: 0.0013049510307610035\n",
      "Iteration: 151 | Loss: 0.0012649553827941418\n",
      "Iteration: 152 | Loss: 0.0012263284297659993\n",
      "Iteration: 153 | Loss: 0.0011890226742252707\n",
      "Iteration: 154 | Loss: 0.0011529934126883745\n",
      "Iteration: 155 | Loss: 0.0011181965237483382\n",
      "Iteration: 156 | Loss: 0.0010845899814739823\n",
      "Iteration: 157 | Loss: 0.0010521329240873456\n",
      "Iteration: 158 | Loss: 0.0010207862360402942\n",
      "Iteration: 159 | Loss: 0.0009905115002766252\n",
      "Iteration: 160 | Loss: 0.0009612721623852849\n",
      "Iteration: 161 | Loss: 0.000933032832108438\n",
      "Iteration: 162 | Loss: 0.0009057591087184846\n",
      "Iteration: 163 | Loss: 0.0008794178138487041\n",
      "Iteration: 164 | Loss: 0.0008539769914932549\n",
      "Iteration: 165 | Loss: 0.0008294056169688702\n",
      "Iteration: 166 | Loss: 0.0008056739461608231\n",
      "Iteration: 167 | Loss: 0.0007827528170309961\n",
      "Iteration: 168 | Loss: 0.0007606144645251334\n",
      "Iteration: 169 | Loss: 0.0007392317056655884\n",
      "Iteration: 170 | Loss: 0.0007185785216279328\n",
      "Iteration: 171 | Loss: 0.0006986295338720083\n",
      "Iteration: 172 | Loss: 0.0006793604698032141\n",
      "Iteration: 173 | Loss: 0.0006607475224882364\n",
      "Iteration: 174 | Loss: 0.0006427679327316582\n",
      "Iteration: 175 | Loss: 0.0006253996980376542\n",
      "Iteration: 176 | Loss: 0.000608621456194669\n",
      "Iteration: 177 | Loss: 0.0005924126016907394\n",
      "Iteration: 178 | Loss: 0.000576753169298172\n",
      "Iteration: 179 | Loss: 0.0005616238340735435\n",
      "Iteration: 180 | Loss: 0.0005470059695653617\n",
      "Iteration: 181 | Loss: 0.0005328817060217261\n",
      "Iteration: 182 | Loss: 0.0005192335811443627\n",
      "Iteration: 183 | Loss: 0.0005060447729192674\n",
      "Iteration: 184 | Loss: 0.0004932990996167064\n",
      "Iteration: 185 | Loss: 0.0004809806705452502\n",
      "Iteration: 186 | Loss: 0.00046907467185519636\n",
      "Iteration: 187 | Loss: 0.0004575661150738597\n",
      "Iteration: 188 | Loss: 0.0004464410594664514\n",
      "Iteration: 189 | Loss: 0.0004356857971288264\n",
      "Iteration: 190 | Loss: 0.0004252872313372791\n",
      "Iteration: 191 | Loss: 0.0004152324399910867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 192 | Loss: 0.00040550928679294884\n",
      "Iteration: 193 | Loss: 0.0003961058973800391\n",
      "Iteration: 194 | Loss: 0.00038701086305081844\n",
      "Iteration: 195 | Loss: 0.00037821292062290013\n",
      "Iteration: 196 | Loss: 0.00036970165092498064\n",
      "Iteration: 197 | Loss: 0.00036146663478575647\n",
      "Iteration: 198 | Loss: 0.0003534980060067028\n",
      "Iteration: 199 | Loss: 0.0003457861312199384\n",
      "Iteration: 200 | Loss: 0.000338321813615039\n",
      "Iteration: 201 | Loss: 0.0003310962056275457\n",
      "Iteration: 202 | Loss: 0.00032410057610832155\n",
      "Iteration: 203 | Loss: 0.00031732674688100815\n",
      "Iteration: 204 | Loss: 0.00031076674349606037\n",
      "Iteration: 205 | Loss: 0.0003044128534384072\n",
      "Iteration: 206 | Loss: 0.00029825756791979074\n",
      "Iteration: 207 | Loss: 0.0002922939311247319\n",
      "Iteration: 208 | Loss: 0.00028651481261476874\n",
      "Iteration: 209 | Loss: 0.0002809138095472008\n",
      "Iteration: 210 | Loss: 0.0002754843735601753\n",
      "Iteration: 211 | Loss: 0.0002702203346416354\n",
      "Iteration: 212 | Loss: 0.0002651158138178289\n",
      "Iteration: 213 | Loss: 0.00026016507763415575\n",
      "Iteration: 214 | Loss: 0.0002553626545704901\n",
      "Iteration: 215 | Loss: 0.0002507031604181975\n",
      "Iteration: 216 | Loss: 0.00024618153111077845\n",
      "Iteration: 217 | Loss: 0.00024179283354897052\n",
      "Iteration: 218 | Loss: 0.0002375323383603245\n",
      "Iteration: 219 | Loss: 0.00023339544713962823\n",
      "Iteration: 220 | Loss: 0.0002293777943123132\n",
      "Iteration: 221 | Loss: 0.00022547514527104795\n",
      "Iteration: 222 | Loss: 0.0002216834109276533\n",
      "Iteration: 223 | Loss: 0.00021799872047267854\n",
      "Iteration: 224 | Loss: 0.00021441721764858812\n",
      "Iteration: 225 | Loss: 0.00021093532268423587\n",
      "Iteration: 226 | Loss: 0.0002075495576718822\n",
      "Iteration: 227 | Loss: 0.00020425647380761802\n",
      "Iteration: 228 | Loss: 0.00020105291332583874\n",
      "Iteration: 229 | Loss: 0.00019793574756477028\n",
      "Iteration: 230 | Loss: 0.00019490192062221467\n",
      "Iteration: 231 | Loss: 0.00019194858032278717\n",
      "Iteration: 232 | Loss: 0.0001890729326987639\n",
      "Iteration: 233 | Loss: 0.00018627234385348856\n",
      "Iteration: 234 | Loss: 0.00018354416533838958\n",
      "Iteration: 235 | Loss: 0.00018088598153553903\n",
      "Iteration: 236 | Loss: 0.0001782953622750938\n",
      "Iteration: 237 | Loss: 0.0001757700665621087\n",
      "Iteration: 238 | Loss: 0.0001733078242978081\n",
      "Iteration: 239 | Loss: 0.00017090654000639915\n",
      "Iteration: 240 | Loss: 0.0001685641473159194\n",
      "Iteration: 241 | Loss: 0.00016627873992547393\n",
      "Iteration: 242 | Loss: 0.0001640483533265069\n",
      "Iteration: 243 | Loss: 0.00016187119763344526\n",
      "Iteration: 244 | Loss: 0.00015974549751263112\n",
      "Iteration: 245 | Loss: 0.00015766957949381322\n",
      "Iteration: 246 | Loss: 0.0001556417701067403\n",
      "Iteration: 247 | Loss: 0.00015366058505605906\n",
      "Iteration: 248 | Loss: 0.0001517244818387553\n",
      "Iteration: 249 | Loss: 0.0001498320052633062\n",
      "Iteration: 250 | Loss: 0.00014798171469010413\n",
      "Iteration: 251 | Loss: 0.00014617237320635468\n",
      "Iteration: 252 | Loss: 0.00014440258382819593\n",
      "Iteration: 253 | Loss: 0.00014267115329857916\n",
      "Iteration: 254 | Loss: 0.00014097685925662518\n",
      "Iteration: 255 | Loss: 0.00013931855210103095\n",
      "Iteration: 256 | Loss: 0.00013769515499006957\n",
      "Iteration: 257 | Loss: 0.00013610554742626846\n",
      "Iteration: 258 | Loss: 0.00013454872532747686\n",
      "Iteration: 259 | Loss: 0.00013302367005962878\n",
      "Iteration: 260 | Loss: 0.00013152946485206485\n",
      "Iteration: 261 | Loss: 0.00013006513472646475\n",
      "Iteration: 262 | Loss: 0.00012862983567174524\n",
      "Iteration: 263 | Loss: 0.00012722266546916217\n",
      "Iteration: 264 | Loss: 0.00012584288197103888\n",
      "Iteration: 265 | Loss: 0.00012448959751054645\n",
      "Iteration: 266 | Loss: 0.00012316209904383868\n",
      "Iteration: 267 | Loss: 0.00012185964442323893\n",
      "Iteration: 268 | Loss: 0.00012058149150107056\n",
      "Iteration: 269 | Loss: 0.00011932697816519067\n",
      "Iteration: 270 | Loss: 0.00011809544957941398\n",
      "Iteration: 271 | Loss: 0.00011688623635564\n",
      "Iteration: 272 | Loss: 0.00011569874914130196\n",
      "Iteration: 273 | Loss: 0.0001145323840319179\n",
      "Iteration: 274 | Loss: 0.0001133865662268363\n",
      "Iteration: 275 | Loss: 0.00011226075730519369\n",
      "Iteration: 276 | Loss: 0.00011115438246633857\n",
      "Iteration: 277 | Loss: 0.00011006698332494125\n",
      "Iteration: 278 | Loss: 0.00010899801418418065\n",
      "Iteration: 279 | Loss: 0.00010794700938276947\n",
      "Iteration: 280 | Loss: 0.00010691351781133562\n",
      "Iteration: 281 | Loss: 0.00010589708108454943\n",
      "Iteration: 282 | Loss: 0.00010489727719686925\n",
      "Iteration: 283 | Loss: 0.00010391366959083825\n",
      "Iteration: 284 | Loss: 0.00010294585081283003\n",
      "Iteration: 285 | Loss: 0.00010199345706496388\n",
      "Iteration: 286 | Loss: 0.0001010560808936134\n",
      "Iteration: 287 | Loss: 0.00010013338032877073\n",
      "Iteration: 288 | Loss: 9.922497702063993e-05\n",
      "Iteration: 289 | Loss: 9.83305653790012e-05\n",
      "Iteration: 290 | Loss: 9.7449759778101e-05\n",
      "Iteration: 291 | Loss: 9.65822982834652e-05\n",
      "Iteration: 292 | Loss: 9.572784620104358e-05\n",
      "Iteration: 293 | Loss: 9.48860906646587e-05\n",
      "Iteration: 294 | Loss: 9.405676246387884e-05\n",
      "Iteration: 295 | Loss: 9.323957056039944e-05\n",
      "Iteration: 296 | Loss: 9.243423119187355e-05\n",
      "Iteration: 297 | Loss: 9.164049697574228e-05\n",
      "Iteration: 298 | Loss: 9.085810597753152e-05\n",
      "Iteration: 299 | Loss: 9.008679626276717e-05\n",
      "Iteration: 300 | Loss: 8.93263568286784e-05\n",
      "Iteration: 301 | Loss: 8.857654756866395e-05\n",
      "Iteration: 302 | Loss: 8.783712837612256e-05\n",
      "Iteration: 303 | Loss: 8.710786642041057e-05\n",
      "Iteration: 304 | Loss: 8.638857252663001e-05\n",
      "Iteration: 305 | Loss: 8.567904296796769e-05\n",
      "Iteration: 306 | Loss: 8.49790740176104e-05\n",
      "Iteration: 307 | Loss: 8.428844739682972e-05\n",
      "Iteration: 308 | Loss: 8.360698848264292e-05\n",
      "Iteration: 309 | Loss: 8.293452265206724e-05\n",
      "Iteration: 310 | Loss: 8.227086073020473e-05\n",
      "Iteration: 311 | Loss: 8.161584264598787e-05\n",
      "Iteration: 312 | Loss: 8.096927194856107e-05\n",
      "Iteration: 313 | Loss: 8.033100311877206e-05\n",
      "Iteration: 314 | Loss: 7.970087608555332e-05\n",
      "Iteration: 315 | Loss: 7.907873077783734e-05\n",
      "Iteration: 316 | Loss: 7.84644071245566e-05\n",
      "Iteration: 317 | Loss: 7.785778143443167e-05\n",
      "Iteration: 318 | Loss: 7.725869363639504e-05\n",
      "Iteration: 319 | Loss: 7.666701276320964e-05\n",
      "Iteration: 320 | Loss: 7.60825932957232e-05\n",
      "Iteration: 321 | Loss: 7.550528971478343e-05\n",
      "Iteration: 322 | Loss: 7.493501470889896e-05\n",
      "Iteration: 323 | Loss: 7.437160820700228e-05\n",
      "Iteration: 324 | Loss: 7.381496106972918e-05\n",
      "Iteration: 325 | Loss: 7.326494960580021e-05\n",
      "Iteration: 326 | Loss: 7.272143557202071e-05\n",
      "Iteration: 327 | Loss: 7.218434620881453e-05\n",
      "Iteration: 328 | Loss: 7.165355782490224e-05\n",
      "Iteration: 329 | Loss: 7.112894672900438e-05\n",
      "Iteration: 330 | Loss: 7.061041833367199e-05\n",
      "Iteration: 331 | Loss: 7.009785622358322e-05\n",
      "Iteration: 332 | Loss: 6.959118763916194e-05\n",
      "Iteration: 333 | Loss: 6.909028888912871e-05\n",
      "Iteration: 334 | Loss: 6.859507993794978e-05\n",
      "Iteration: 335 | Loss: 6.810545164626092e-05\n",
      "Iteration: 336 | Loss: 6.7621331254486e-05\n",
      "Iteration: 337 | Loss: 6.71426096232608e-05\n",
      "Iteration: 338 | Loss: 6.666920671705157e-05\n",
      "Iteration: 339 | Loss: 6.620103522436693e-05\n",
      "Iteration: 340 | Loss: 6.573802966158837e-05\n",
      "Iteration: 341 | Loss: 6.528008088935167e-05\n",
      "Iteration: 342 | Loss: 6.482713797595352e-05\n",
      "Iteration: 343 | Loss: 6.437909905798733e-05\n",
      "Iteration: 344 | Loss: 6.393587682396173e-05\n",
      "Iteration: 345 | Loss: 6.349742034217343e-05\n",
      "Iteration: 346 | Loss: 6.306366412900388e-05\n",
      "Iteration: 347 | Loss: 6.263451359700412e-05\n",
      "Iteration: 348 | Loss: 6.220991053851321e-05\n",
      "Iteration: 349 | Loss: 6.178978219395503e-05\n",
      "Iteration: 350 | Loss: 6.137406307971105e-05\n",
      "Iteration: 351 | Loss: 6.0962684074183926e-05\n",
      "Iteration: 352 | Loss: 6.055557241779752e-05\n",
      "Iteration: 353 | Loss: 6.015268445480615e-05\n",
      "Iteration: 354 | Loss: 5.97539619775489e-05\n",
      "Iteration: 355 | Loss: 5.9359306760597974e-05\n",
      "Iteration: 356 | Loss: 5.896869333810173e-05\n",
      "Iteration: 357 | Loss: 5.858205622644164e-05\n",
      "Iteration: 358 | Loss: 5.8199333579977974e-05\n",
      "Iteration: 359 | Loss: 5.782046719104983e-05\n",
      "Iteration: 360 | Loss: 5.7445413403911516e-05\n",
      "Iteration: 361 | Loss: 5.70741030969657e-05\n",
      "Iteration: 362 | Loss: 5.670648897648789e-05\n",
      "Iteration: 363 | Loss: 5.6342520110774785e-05\n",
      "Iteration: 364 | Loss: 5.5982149206101894e-05\n",
      "Iteration: 365 | Loss: 5.562532533076592e-05\n",
      "Iteration: 366 | Loss: 5.5271997553063557e-05\n",
      "Iteration: 367 | Loss: 5.4922111303312704e-05\n",
      "Iteration: 368 | Loss: 5.457563020172529e-05\n",
      "Iteration: 369 | Loss: 5.423250331659801e-05\n",
      "Iteration: 370 | Loss: 5.3892668802291155e-05\n",
      "Iteration: 371 | Loss: 5.3556115744868293e-05\n",
      "Iteration: 372 | Loss: 5.32227786607109e-05\n",
      "Iteration: 373 | Loss: 5.2892610256094486e-05\n",
      "Iteration: 374 | Loss: 5.256557778920978e-05\n",
      "Iteration: 375 | Loss: 5.224164488026872e-05\n",
      "Iteration: 376 | Loss: 5.19207569595892e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 377 | Loss: 5.160288492334075e-05\n",
      "Iteration: 378 | Loss: 5.128797056386247e-05\n",
      "Iteration: 379 | Loss: 5.097600296721794e-05\n",
      "Iteration: 380 | Loss: 5.066692392574623e-05\n",
      "Iteration: 381 | Loss: 5.036068978370167e-05\n",
      "Iteration: 382 | Loss: 5.005730054108426e-05\n",
      "Iteration: 383 | Loss: 4.9756676162360236e-05\n",
      "Iteration: 384 | Loss: 4.945880937157199e-05\n",
      "Iteration: 385 | Loss: 4.9163649237016216e-05\n",
      "Iteration: 386 | Loss: 4.887117393082008e-05\n",
      "Iteration: 387 | Loss: 4.858132888330147e-05\n",
      "Iteration: 388 | Loss: 4.829411409446038e-05\n",
      "Iteration: 389 | Loss: 4.800947135663591e-05\n",
      "Iteration: 390 | Loss: 4.7727364290039986e-05\n",
      "Iteration: 391 | Loss: 4.7447778342757374e-05\n",
      "Iteration: 392 | Loss: 4.717068077297881e-05\n",
      "Iteration: 393 | Loss: 4.689604975283146e-05\n",
      "Iteration: 394 | Loss: 4.662381616071798e-05\n",
      "Iteration: 395 | Loss: 4.635397999663837e-05\n",
      "Iteration: 396 | Loss: 4.608651215676218e-05\n",
      "Iteration: 397 | Loss: 4.5821379899280146e-05\n",
      "Iteration: 398 | Loss: 4.5558550482383e-05\n",
      "Iteration: 399 | Loss: 4.529801299213432e-05\n",
      "Iteration: 400 | Loss: 4.503970922087319e-05\n",
      "Iteration: 401 | Loss: 4.478363916859962e-05\n",
      "Iteration: 402 | Loss: 4.4529773731483147e-05\n",
      "Iteration: 403 | Loss: 4.4278058339841664e-05\n",
      "Iteration: 404 | Loss: 4.4028500269632787e-05\n",
      "Iteration: 405 | Loss: 4.3781074055004865e-05\n",
      "Iteration: 406 | Loss: 4.35357287642546e-05\n",
      "Iteration: 407 | Loss: 4.32924680353608e-05\n",
      "Iteration: 408 | Loss: 4.305124457459897e-05\n",
      "Iteration: 409 | Loss: 4.281204746803269e-05\n",
      "Iteration: 410 | Loss: 4.257485852576792e-05\n",
      "Iteration: 411 | Loss: 4.2339623178122565e-05\n",
      "Iteration: 412 | Loss: 4.210635233903304e-05\n",
      "Iteration: 413 | Loss: 4.1875013266690075e-05\n",
      "Iteration: 414 | Loss: 4.164559140917845e-05\n",
      "Iteration: 415 | Loss: 4.1418046748731285e-05\n",
      "Iteration: 416 | Loss: 4.1192372009390965e-05\n",
      "Iteration: 417 | Loss: 4.096854536328465e-05\n",
      "Iteration: 418 | Loss: 4.074654134456068e-05\n",
      "Iteration: 419 | Loss: 4.0526341763325036e-05\n",
      "Iteration: 420 | Loss: 4.030791751574725e-05\n",
      "Iteration: 421 | Loss: 4.0091254049912095e-05\n",
      "Iteration: 422 | Loss: 3.987634045188315e-05\n",
      "Iteration: 423 | Loss: 3.966315489378758e-05\n",
      "Iteration: 424 | Loss: 3.9451679185731336e-05\n",
      "Iteration: 425 | Loss: 3.924187330994755e-05\n",
      "Iteration: 426 | Loss: 3.9033744542393833e-05\n",
      "Iteration: 427 | Loss: 3.8827267417218536e-05\n",
      "Iteration: 428 | Loss: 3.8622420106548816e-05\n",
      "Iteration: 429 | Loss: 3.841918442049064e-05\n",
      "Iteration: 430 | Loss: 3.821754580712877e-05\n",
      "Iteration: 431 | Loss: 3.80174933525268e-05\n",
      "Iteration: 432 | Loss: 3.7818994314875454e-05\n",
      "Iteration: 433 | Loss: 3.762203414225951e-05\n",
      "Iteration: 434 | Loss: 3.742661647265777e-05\n",
      "Iteration: 435 | Loss: 3.723270128830336e-05\n",
      "Iteration: 436 | Loss: 3.7040281313238665e-05\n",
      "Iteration: 437 | Loss: 3.6849356547463685e-05\n",
      "Iteration: 438 | Loss: 3.665987969725393e-05\n",
      "Iteration: 439 | Loss: 3.6471858038567007e-05\n",
      "Iteration: 440 | Loss: 3.628526974353008e-05\n",
      "Iteration: 441 | Loss: 3.6100103898206726e-05\n",
      "Iteration: 442 | Loss: 3.591634231270291e-05\n",
      "Iteration: 443 | Loss: 3.5733966797124594e-05\n",
      "Iteration: 444 | Loss: 3.5552959161577746e-05\n",
      "Iteration: 445 | Loss: 3.5373319406062365e-05\n",
      "Iteration: 446 | Loss: 3.51950220647268e-05\n",
      "Iteration: 447 | Loss: 3.501807077554986e-05\n",
      "Iteration: 448 | Loss: 3.484242915874347e-05\n",
      "Iteration: 449 | Loss: 3.4668086300371215e-05\n",
      "Iteration: 450 | Loss: 3.449503856245428e-05\n",
      "Iteration: 451 | Loss: 3.432327866903506e-05\n",
      "Iteration: 452 | Loss: 3.4152777516283095e-05\n",
      "Iteration: 453 | Loss: 3.398353874217719e-05\n",
      "Iteration: 454 | Loss: 3.381552596692927e-05\n",
      "Iteration: 455 | Loss: 3.3648750104475766e-05\n",
      "Iteration: 456 | Loss: 3.3483182050986215e-05\n",
      "Iteration: 457 | Loss: 3.331882908241823e-05\n",
      "Iteration: 458 | Loss: 3.3155658456962556e-05\n",
      "Iteration: 459 | Loss: 3.29936774505768e-05\n",
      "Iteration: 460 | Loss: 3.2832864235388115e-05\n",
      "Iteration: 461 | Loss: 3.2673200621502474e-05\n",
      "Iteration: 462 | Loss: 3.2514686608919874e-05\n",
      "Iteration: 463 | Loss: 3.235730400774628e-05\n",
      "Iteration: 464 | Loss: 3.2201045542024076e-05\n",
      "Iteration: 465 | Loss: 3.204590393579565e-05\n",
      "Iteration: 466 | Loss: 3.189185372320935e-05\n",
      "Iteration: 467 | Loss: 3.173889490426518e-05\n",
      "Iteration: 468 | Loss: 3.158702020300552e-05\n",
      "Iteration: 469 | Loss: 3.143621142953634e-05\n",
      "Iteration: 470 | Loss: 3.1286464945878834e-05\n",
      "Iteration: 471 | Loss: 3.1137769838096574e-05\n",
      "Iteration: 472 | Loss: 3.099010791629553e-05\n",
      "Iteration: 473 | Loss: 3.084347554249689e-05\n",
      "Iteration: 474 | Loss: 3.069786180276424e-05\n",
      "Iteration: 475 | Loss: 3.055326305911876e-05\n",
      "Iteration: 476 | Loss: 3.0409657483687624e-05\n",
      "Iteration: 477 | Loss: 3.0267046895460226e-05\n",
      "Iteration: 478 | Loss: 3.012541310454253e-05\n",
      "Iteration: 479 | Loss: 2.9984763386892155e-05\n",
      "Iteration: 480 | Loss: 2.9845061362721026e-05\n",
      "Iteration: 481 | Loss: 2.9706317945965566e-05\n",
      "Iteration: 482 | Loss: 2.9568516765721142e-05\n",
      "Iteration: 483 | Loss: 2.9431657821987756e-05\n",
      "Iteration: 484 | Loss: 2.9295726562850177e-05\n",
      "Iteration: 485 | Loss: 2.9160704798414372e-05\n",
      "Iteration: 486 | Loss: 2.9026607080595568e-05\n",
      "Iteration: 487 | Loss: 2.8893398848595098e-05\n",
      "Iteration: 488 | Loss: 2.876109465432819e-05\n",
      "Iteration: 489 | Loss: 2.8629672669922e-05\n",
      "Iteration: 490 | Loss: 2.8499123800429516e-05\n",
      "Iteration: 491 | Loss: 2.8369444407871924e-05\n",
      "Iteration: 492 | Loss: 2.8240627216291614e-05\n",
      "Iteration: 493 | Loss: 2.8112668587709777e-05\n",
      "Iteration: 494 | Loss: 2.7985548513242975e-05\n",
      "Iteration: 495 | Loss: 2.785927790682763e-05\n",
      "Iteration: 496 | Loss: 2.7733833121601492e-05\n",
      "Iteration: 497 | Loss: 2.760921051958576e-05\n",
      "Iteration: 498 | Loss: 2.748540646280162e-05\n",
      "Iteration: 499 | Loss: 2.7362402761355042e-05\n",
      "Iteration: 500 | Loss: 2.724021214817185e-05\n",
      "Iteration: 501 | Loss: 2.71188127953792e-05\n",
      "Iteration: 502 | Loss: 2.6998201064998284e-05\n",
      "Iteration: 503 | Loss: 2.6878362405113876e-05\n",
      "Iteration: 504 | Loss: 2.67593095486518e-05\n",
      "Iteration: 505 | Loss: 2.66410133917816e-05\n",
      "Iteration: 506 | Loss: 2.65234884864185e-05\n",
      "Iteration: 507 | Loss: 2.640671118570026e-05\n",
      "Iteration: 508 | Loss: 2.6290679670637473e-05\n",
      "Iteration: 509 | Loss: 2.6175384846283123e-05\n",
      "Iteration: 510 | Loss: 2.6060830350616015e-05\n",
      "Iteration: 511 | Loss: 2.594699981273152e-05\n",
      "Iteration: 512 | Loss: 2.5833895051619038e-05\n",
      "Iteration: 513 | Loss: 2.5721501515363343e-05\n",
      "Iteration: 514 | Loss: 2.560982284194324e-05\n",
      "Iteration: 515 | Loss: 2.5498842660454102e-05\n",
      "Iteration: 516 | Loss: 2.538856460887473e-05\n",
      "Iteration: 517 | Loss: 2.5278970497311093e-05\n",
      "Iteration: 518 | Loss: 2.5170069420710206e-05\n",
      "Iteration: 519 | Loss: 2.5061837732209824e-05\n",
      "Iteration: 520 | Loss: 2.4954284526756965e-05\n",
      "Iteration: 521 | Loss: 2.484740070940461e-05\n",
      "Iteration: 522 | Loss: 2.4741179004195146e-05\n",
      "Iteration: 523 | Loss: 2.4635615773149766e-05\n",
      "Iteration: 524 | Loss: 2.4530698283342645e-05\n",
      "Iteration: 525 | Loss: 2.4426437448710203e-05\n",
      "Iteration: 526 | Loss: 2.4322807803400792e-05\n",
      "Iteration: 527 | Loss: 2.4219816623372026e-05\n",
      "Iteration: 528 | Loss: 2.4117454813676886e-05\n",
      "Iteration: 529 | Loss: 2.4015718736336567e-05\n",
      "Iteration: 530 | Loss: 2.3914601115393452e-05\n",
      "Iteration: 531 | Loss: 2.381410013185814e-05\n",
      "Iteration: 532 | Loss: 2.3714206690783612e-05\n",
      "Iteration: 533 | Loss: 2.361492079216987e-05\n",
      "Iteration: 534 | Loss: 2.3516224246122874e-05\n",
      "Iteration: 535 | Loss: 2.3418135242536664e-05\n",
      "Iteration: 536 | Loss: 2.3320631953538395e-05\n",
      "Iteration: 537 | Loss: 2.322371074114926e-05\n",
      "Iteration: 538 | Loss: 2.3127377062337473e-05\n",
      "Iteration: 539 | Loss: 2.303161090821959e-05\n",
      "Iteration: 540 | Loss: 2.2936412278795615e-05\n",
      "Iteration: 541 | Loss: 2.2841790269012563e-05\n",
      "Iteration: 542 | Loss: 2.2747728507965803e-05\n",
      "Iteration: 543 | Loss: 2.2654221538687125e-05\n",
      "Iteration: 544 | Loss: 2.2561267542187124e-05\n",
      "Iteration: 545 | Loss: 2.246886106149759e-05\n",
      "Iteration: 546 | Loss: 2.2377005734597333e-05\n",
      "Iteration: 547 | Loss: 2.2285683371592313e-05\n",
      "Iteration: 548 | Loss: 2.219489215349313e-05\n",
      "Iteration: 549 | Loss: 2.2104635718278587e-05\n",
      "Iteration: 550 | Loss: 2.2014917703927495e-05\n",
      "Iteration: 551 | Loss: 2.1925705368630588e-05\n",
      "Iteration: 552 | Loss: 2.1837022359250113e-05\n",
      "Iteration: 553 | Loss: 2.1748850485892035e-05\n",
      "Iteration: 554 | Loss: 2.1661189748556353e-05\n",
      "Iteration: 555 | Loss: 2.157403650926426e-05\n",
      "Iteration: 556 | Loss: 2.1487385311047547e-05\n",
      "Iteration: 557 | Loss: 2.140123615390621e-05\n",
      "Iteration: 558 | Loss: 2.131558176188264e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 559 | Loss: 2.1230418496998027e-05\n",
      "Iteration: 560 | Loss: 2.1145744540262967e-05\n",
      "Iteration: 561 | Loss: 2.1061545339762233e-05\n",
      "Iteration: 562 | Loss: 2.0977831809432246e-05\n",
      "Iteration: 563 | Loss: 2.0894598492304794e-05\n",
      "Iteration: 564 | Loss: 2.081183083646465e-05\n",
      "Iteration: 565 | Loss: 2.0729532479890622e-05\n",
      "Iteration: 566 | Loss: 2.0647707060561515e-05\n",
      "Iteration: 567 | Loss: 2.0566336388583295e-05\n",
      "Iteration: 568 | Loss: 2.048542955890298e-05\n",
      "Iteration: 569 | Loss: 2.0404973838594742e-05\n",
      "Iteration: 570 | Loss: 2.0324972865637392e-05\n",
      "Iteration: 571 | Loss: 2.0245421183062717e-05\n",
      "Iteration: 572 | Loss: 2.0166307876934297e-05\n",
      "Iteration: 573 | Loss: 2.0087643861188553e-05\n",
      "Iteration: 574 | Loss: 2.000941094593145e-05\n",
      "Iteration: 575 | Loss: 1.9931614588131197e-05\n",
      "Iteration: 576 | Loss: 1.9854260244756006e-05\n",
      "Iteration: 577 | Loss: 1.9777322449954227e-05\n",
      "Iteration: 578 | Loss: 1.9700813936651684e-05\n",
      "Iteration: 579 | Loss: 1.9624729247880168e-05\n",
      "Iteration: 580 | Loss: 1.9549066564650275e-05\n",
      "Iteration: 581 | Loss: 1.9473809516057372e-05\n",
      "Iteration: 582 | Loss: 1.9398967197048478e-05\n",
      "Iteration: 583 | Loss: 1.9324545064591803e-05\n",
      "Iteration: 584 | Loss: 1.9250517652835697e-05\n",
      "Iteration: 585 | Loss: 1.9176903151674196e-05\n",
      "Iteration: 586 | Loss: 1.9103687009192072e-05\n",
      "Iteration: 587 | Loss: 1.903086194943171e-05\n",
      "Iteration: 588 | Loss: 1.895843888632953e-05\n",
      "Iteration: 589 | Loss: 1.888641418190673e-05\n",
      "Iteration: 590 | Loss: 1.8814767827279866e-05\n",
      "Iteration: 591 | Loss: 1.8743514374364167e-05\n",
      "Iteration: 592 | Loss: 1.8672644728212617e-05\n",
      "Iteration: 593 | Loss: 1.8602153431857005e-05\n",
      "Iteration: 594 | Loss: 1.8532036847318523e-05\n",
      "Iteration: 595 | Loss: 1.84623095265124e-05\n",
      "Iteration: 596 | Loss: 1.8392942365608178e-05\n",
      "Iteration: 597 | Loss: 1.8323949916521087e-05\n",
      "Iteration: 598 | Loss: 1.825532308430411e-05\n",
      "Iteration: 599 | Loss: 1.8187061868957244e-05\n",
      "Iteration: 600 | Loss: 1.811916627048049e-05\n",
      "Iteration: 601 | Loss: 1.8051627193926834e-05\n",
      "Iteration: 602 | Loss: 1.7984448277275078e-05\n",
      "Iteration: 603 | Loss: 1.79176167875994e-05\n",
      "Iteration: 604 | Loss: 1.7851145457825623e-05\n",
      "Iteration: 605 | Loss: 1.7785021555027924e-05\n",
      "Iteration: 606 | Loss: 1.7719245079206303e-05\n",
      "Iteration: 607 | Loss: 1.7653819668339565e-05\n",
      "Iteration: 608 | Loss: 1.7588732589501888e-05\n",
      "Iteration: 609 | Loss: 1.752398384269327e-05\n",
      "Iteration: 610 | Loss: 1.745957342791371e-05\n",
      "Iteration: 611 | Loss: 1.7395499526173808e-05\n",
      "Iteration: 612 | Loss: 1.733175668050535e-05\n",
      "Iteration: 613 | Loss: 1.726834670989774e-05\n",
      "Iteration: 614 | Loss: 1.7205262338393368e-05\n",
      "Iteration: 615 | Loss: 1.714250902296044e-05\n",
      "Iteration: 616 | Loss: 1.7080075849662535e-05\n",
      "Iteration: 617 | Loss: 1.701796645647846e-05\n",
      "Iteration: 618 | Loss: 1.695618084340822e-05\n",
      "Iteration: 619 | Loss: 1.6894706277525984e-05\n",
      "Iteration: 620 | Loss: 1.683354639681056e-05\n",
      "Iteration: 621 | Loss: 1.6772706658230163e-05\n",
      "Iteration: 622 | Loss: 1.6712174328858964e-05\n",
      "Iteration: 623 | Loss: 1.6651945770718157e-05\n",
      "Iteration: 624 | Loss: 1.6592028259765357e-05\n",
      "Iteration: 625 | Loss: 1.6532418158021756e-05\n",
      "Iteration: 626 | Loss: 1.647310818952974e-05\n",
      "Iteration: 627 | Loss: 1.6414100173278712e-05\n",
      "Iteration: 628 | Loss: 1.635538683331106e-05\n",
      "Iteration: 629 | Loss: 1.629697362659499e-05\n",
      "Iteration: 630 | Loss: 1.62388569151517e-05\n",
      "Iteration: 631 | Loss: 1.618102760403417e-05\n",
      "Iteration: 632 | Loss: 1.612349115021061e-05\n",
      "Iteration: 633 | Loss: 1.6066242096712813e-05\n",
      "Iteration: 634 | Loss: 1.6009285900508985e-05\n",
      "Iteration: 635 | Loss: 1.59526080096839e-05\n",
      "Iteration: 636 | Loss: 1.5896217519184574e-05\n",
      "Iteration: 637 | Loss: 1.5840105334063992e-05\n",
      "Iteration: 638 | Loss: 1.578427509230096e-05\n",
      "Iteration: 639 | Loss: 1.572871951793786e-05\n",
      "Iteration: 640 | Loss: 1.56734386109747e-05\n",
      "Iteration: 641 | Loss: 1.5618439647369087e-05\n",
      "Iteration: 642 | Loss: 1.5563706256216392e-05\n",
      "Iteration: 643 | Loss: 1.550924571347423e-05\n",
      "Iteration: 644 | Loss: 1.5455052562174387e-05\n",
      "Iteration: 645 | Loss: 1.540112316433806e-05\n",
      "Iteration: 646 | Loss: 1.5347462976933457e-05\n",
      "Iteration: 647 | Loss: 1.529406290501356e-05\n",
      "Iteration: 648 | Loss: 1.524092203908367e-05\n",
      "Iteration: 649 | Loss: 1.5188048564596102e-05\n",
      "Iteration: 650 | Loss: 1.5135427020140924e-05\n",
      "Iteration: 651 | Loss: 1.5083065591170453e-05\n",
      "Iteration: 652 | Loss: 1.503095973021118e-05\n",
      "Iteration: 653 | Loss: 1.4979100342316087e-05\n",
      "Iteration: 654 | Loss: 1.4927501069905702e-05\n",
      "Iteration: 655 | Loss: 1.4876146451570094e-05\n",
      "Iteration: 656 | Loss: 1.4825048310740385e-05\n",
      "Iteration: 657 | Loss: 1.4774186638533138e-05\n",
      "Iteration: 658 | Loss: 1.472357598686358e-05\n",
      "Iteration: 659 | Loss: 1.4673209989268798e-05\n",
      "Iteration: 660 | Loss: 1.4623083188780583e-05\n",
      "Iteration: 661 | Loss: 1.457319831388304e-05\n",
      "Iteration: 662 | Loss: 1.4523554455081467e-05\n",
      "Iteration: 663 | Loss: 1.4474143426923547e-05\n",
      "Iteration: 664 | Loss: 1.4424972505366895e-05\n",
      "Iteration: 665 | Loss: 1.4376039871422108e-05\n",
      "Iteration: 666 | Loss: 1.4327331882668659e-05\n",
      "Iteration: 667 | Loss: 1.4278860362537671e-05\n",
      "Iteration: 668 | Loss: 1.4230616216082126e-05\n",
      "Iteration: 669 | Loss: 1.4182601262291428e-05\n",
      "Iteration: 670 | Loss: 1.4134819139144383e-05\n",
      "Iteration: 671 | Loss: 1.408725256624166e-05\n",
      "Iteration: 672 | Loss: 1.4039922461961396e-05\n",
      "Iteration: 673 | Loss: 1.3992806998430751e-05\n",
      "Iteration: 674 | Loss: 1.3945917999080848e-05\n",
      "Iteration: 675 | Loss: 1.3899249097448774e-05\n",
      "Iteration: 676 | Loss: 1.3852797565050423e-05\n",
      "Iteration: 677 | Loss: 1.3806562492391095e-05\n",
      "Iteration: 678 | Loss: 1.3760543879470788e-05\n",
      "Iteration: 679 | Loss: 1.3714740816794802e-05\n",
      "Iteration: 680 | Loss: 1.3669151485373732e-05\n",
      "Iteration: 681 | Loss: 1.3623778613691684e-05\n",
      "Iteration: 682 | Loss: 1.3578611287812237e-05\n",
      "Iteration: 683 | Loss: 1.3533655874198303e-05\n",
      "Iteration: 684 | Loss: 1.3488909644365776e-05\n",
      "Iteration: 685 | Loss: 1.3444366231851745e-05\n",
      "Iteration: 686 | Loss: 1.3400034731603228e-05\n",
      "Iteration: 687 | Loss: 1.3355906958167907e-05\n",
      "Iteration: 688 | Loss: 1.3311978364072274e-05\n",
      "Iteration: 689 | Loss: 1.3268258044263348e-05\n",
      "Iteration: 690 | Loss: 1.3224732356320601e-05\n",
      "Iteration: 691 | Loss: 1.318141494266456e-05\n",
      "Iteration: 692 | Loss: 1.3138293979864102e-05\n",
      "Iteration: 693 | Loss: 1.3095359463477507e-05\n",
      "Iteration: 694 | Loss: 1.3052631402388215e-05\n",
      "Iteration: 695 | Loss: 1.3010095244680997e-05\n",
      "Iteration: 696 | Loss: 1.2967752809345257e-05\n",
      "Iteration: 697 | Loss: 1.2925606824865099e-05\n",
      "Iteration: 698 | Loss: 1.2883646377304103e-05\n",
      "Iteration: 699 | Loss: 1.2841880561609287e-05\n",
      "Iteration: 700 | Loss: 1.280030392081244e-05\n",
      "Iteration: 701 | Loss: 1.2758914635924157e-05\n",
      "Iteration: 702 | Loss: 1.2717714525933843e-05\n",
      "Iteration: 703 | Loss: 1.2676698133873288e-05\n",
      "Iteration: 704 | Loss: 1.263586545974249e-05\n",
      "Iteration: 705 | Loss: 1.259522105101496e-05\n",
      "Iteration: 706 | Loss: 1.255475672223838e-05\n",
      "Iteration: 707 | Loss: 1.2514469744928647e-05\n",
      "Iteration: 708 | Loss: 1.2474369214032777e-05\n",
      "Iteration: 709 | Loss: 1.2434446944098454e-05\n",
      "Iteration: 710 | Loss: 1.2394702025630977e-05\n",
      "Iteration: 711 | Loss: 1.2355135368125048e-05\n",
      "Iteration: 712 | Loss: 1.2315742424107157e-05\n",
      "Iteration: 713 | Loss: 1.2276529560040217e-05\n",
      "Iteration: 714 | Loss: 1.2237486771482509e-05\n",
      "Iteration: 715 | Loss: 1.219861860590754e-05\n",
      "Iteration: 716 | Loss: 1.2159924153820612e-05\n",
      "Iteration: 717 | Loss: 1.2121399777242914e-05\n",
      "Iteration: 718 | Loss: 1.2083043657185044e-05\n",
      "Iteration: 719 | Loss: 1.2044858522131108e-05\n",
      "Iteration: 720 | Loss: 1.2006842553091701e-05\n",
      "Iteration: 721 | Loss: 1.1968994840572122e-05\n",
      "Iteration: 722 | Loss: 1.1931310837098863e-05\n",
      "Iteration: 723 | Loss: 1.1893795090145431e-05\n",
      "Iteration: 724 | Loss: 1.185643850476481e-05\n",
      "Iteration: 725 | Loss: 1.1819252904388122e-05\n",
      "Iteration: 726 | Loss: 1.1782228284573648e-05\n",
      "Iteration: 727 | Loss: 1.174536555481609e-05\n",
      "Iteration: 728 | Loss: 1.1708660167641938e-05\n",
      "Iteration: 729 | Loss: 1.1672113032545894e-05\n",
      "Iteration: 730 | Loss: 1.1635726878012065e-05\n",
      "Iteration: 731 | Loss: 1.1599502613535151e-05\n",
      "Iteration: 732 | Loss: 1.1563434782146942e-05\n",
      "Iteration: 733 | Loss: 1.1527518836373929e-05\n",
      "Iteration: 734 | Loss: 1.149175841419492e-05\n",
      "Iteration: 735 | Loss: 1.1456153515609913e-05\n",
      "Iteration: 736 | Loss: 1.1420706869103014e-05\n",
      "Iteration: 737 | Loss: 1.1385408470232505e-05\n",
      "Iteration: 738 | Loss: 1.1350261956977192e-05\n",
      "Iteration: 739 | Loss: 1.1315269148326479e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 740 | Loss: 1.128042185882805e-05\n",
      "Iteration: 741 | Loss: 1.1245725545450114e-05\n",
      "Iteration: 742 | Loss: 1.1211182936676778e-05\n",
      "Iteration: 743 | Loss: 1.1176786756550428e-05\n",
      "Iteration: 744 | Loss: 1.1142531548102852e-05\n",
      "Iteration: 745 | Loss: 1.1108424587291665e-05\n",
      "Iteration: 746 | Loss: 1.1074467693106271e-05\n",
      "Iteration: 747 | Loss: 1.1040646313631441e-05\n",
      "Iteration: 748 | Loss: 1.1006975910277106e-05\n",
      "Iteration: 749 | Loss: 1.0973445569106843e-05\n",
      "Iteration: 750 | Loss: 1.0940060747088864e-05\n",
      "Iteration: 751 | Loss: 1.0906812349276152e-05\n",
      "Iteration: 752 | Loss: 1.0873710380110424e-05\n",
      "Iteration: 753 | Loss: 1.0840742106665857e-05\n",
      "Iteration: 754 | Loss: 1.080791753338417e-05\n",
      "Iteration: 755 | Loss: 1.0775227565318346e-05\n",
      "Iteration: 756 | Loss: 1.0742673111963086e-05\n",
      "Iteration: 757 | Loss: 1.0710260539781302e-05\n",
      "Iteration: 758 | Loss: 1.0677980753825977e-05\n",
      "Iteration: 759 | Loss: 1.0645837392075919e-05\n",
      "Iteration: 760 | Loss: 1.0613830454531126e-05\n",
      "Iteration: 761 | Loss: 1.0581954484223388e-05\n",
      "Iteration: 762 | Loss: 1.0550209481152706e-05\n",
      "Iteration: 763 | Loss: 1.0518601811781991e-05\n",
      "Iteration: 764 | Loss: 1.0487123290658928e-05\n",
      "Iteration: 765 | Loss: 1.0455777555762324e-05\n",
      "Iteration: 766 | Loss: 1.0424559150123969e-05\n",
      "Iteration: 767 | Loss: 1.0393470802227966e-05\n",
      "Iteration: 768 | Loss: 1.0362510693084914e-05\n",
      "Iteration: 769 | Loss: 1.0331679732189514e-05\n",
      "Iteration: 770 | Loss: 1.0300978829036467e-05\n",
      "Iteration: 771 | Loss: 1.0270402526657563e-05\n",
      "Iteration: 772 | Loss: 1.023995537252631e-05\n",
      "Iteration: 773 | Loss: 1.0209625543211587e-05\n",
      "Iteration: 774 | Loss: 1.0179423043155111e-05\n",
      "Iteration: 775 | Loss: 1.0149347872356884e-05\n",
      "Iteration: 776 | Loss: 1.0119392754859291e-05\n",
      "Iteration: 777 | Loss: 1.0089563147630543e-05\n",
      "Iteration: 778 | Loss: 1.0059857231681235e-05\n",
      "Iteration: 779 | Loss: 1.0030269550043158e-05\n",
      "Iteration: 780 | Loss: 1.0000801012211014e-05\n",
      "Iteration: 781 | Loss: 9.97145616565831e-06\n",
      "Iteration: 782 | Loss: 9.942226824932732e-06\n",
      "Iteration: 783 | Loss: 9.913118447002489e-06\n",
      "Iteration: 784 | Loss: 9.884129212878179e-06\n",
      "Iteration: 785 | Loss: 9.855255484580994e-06\n",
      "Iteration: 786 | Loss: 9.826501809584443e-06\n",
      "Iteration: 787 | Loss: 9.797859092941508e-06\n",
      "Iteration: 788 | Loss: 9.769340977072716e-06\n",
      "Iteration: 789 | Loss: 9.740931091073435e-06\n",
      "Iteration: 790 | Loss: 9.71263762039598e-06\n",
      "Iteration: 791 | Loss: 9.684457836556248e-06\n",
      "Iteration: 792 | Loss: 9.656393558543641e-06\n",
      "Iteration: 793 | Loss: 9.62844023888465e-06\n",
      "Iteration: 794 | Loss: 9.600598787073977e-06\n",
      "Iteration: 795 | Loss: 9.572867384122219e-06\n",
      "Iteration: 796 | Loss: 9.545249668008182e-06\n",
      "Iteration: 797 | Loss: 9.51774200075306e-06\n",
      "Iteration: 798 | Loss: 9.490342563367449e-06\n",
      "Iteration: 799 | Loss: 9.463054084335454e-06\n",
      "Iteration: 800 | Loss: 9.435875654162373e-06\n",
      "Iteration: 801 | Loss: 9.408800906385295e-06\n",
      "Iteration: 802 | Loss: 9.381838935951237e-06\n",
      "Iteration: 803 | Loss: 9.354981557407882e-06\n",
      "Iteration: 804 | Loss: 9.328229680249933e-06\n",
      "Iteration: 805 | Loss: 9.301586032961495e-06\n",
      "Iteration: 806 | Loss: 9.27504152059555e-06\n",
      "Iteration: 807 | Loss: 9.248610695067327e-06\n",
      "Iteration: 808 | Loss: 9.222282642440405e-06\n",
      "Iteration: 809 | Loss: 9.19605645322008e-06\n",
      "Iteration: 810 | Loss: 9.169930308416951e-06\n",
      "Iteration: 811 | Loss: 9.143913302978035e-06\n",
      "Iteration: 812 | Loss: 9.11799452296691e-06\n",
      "Iteration: 813 | Loss: 9.092177606362384e-06\n",
      "Iteration: 814 | Loss: 9.066462553164456e-06\n",
      "Iteration: 815 | Loss: 9.04085118236253e-06\n",
      "Iteration: 816 | Loss: 9.015337127493694e-06\n",
      "Iteration: 817 | Loss: 8.989919479063246e-06\n",
      "Iteration: 818 | Loss: 8.964608241512906e-06\n",
      "Iteration: 819 | Loss: 8.939387953432743e-06\n",
      "Iteration: 820 | Loss: 8.91427043825388e-06\n",
      "Iteration: 821 | Loss: 8.889247510524001e-06\n",
      "Iteration: 822 | Loss: 8.864328265190125e-06\n",
      "Iteration: 823 | Loss: 8.839501788315829e-06\n",
      "Iteration: 824 | Loss: 8.814768079901114e-06\n",
      "Iteration: 825 | Loss: 8.790135325398296e-06\n",
      "Iteration: 826 | Loss: 8.765594429860357e-06\n",
      "Iteration: 827 | Loss: 8.741149031266104e-06\n",
      "Iteration: 828 | Loss: 8.71680003911024e-06\n",
      "Iteration: 829 | Loss: 8.692546543898061e-06\n",
      "Iteration: 830 | Loss: 8.668380360177252e-06\n",
      "Iteration: 831 | Loss: 8.644311492389534e-06\n",
      "Iteration: 832 | Loss: 8.620334483566694e-06\n",
      "Iteration: 833 | Loss: 8.59644751471933e-06\n",
      "Iteration: 834 | Loss: 8.57265513332095e-06\n",
      "Iteration: 835 | Loss: 8.548952791898046e-06\n",
      "Iteration: 836 | Loss: 8.525339580955915e-06\n",
      "Iteration: 837 | Loss: 8.501817319483962e-06\n",
      "Iteration: 838 | Loss: 8.47838327899808e-06\n",
      "Iteration: 839 | Loss: 8.455041097477078e-06\n",
      "Iteration: 840 | Loss: 8.431788955931552e-06\n",
      "Iteration: 841 | Loss: 8.40862139739329e-06\n",
      "Iteration: 842 | Loss: 8.385544788325205e-06\n",
      "Iteration: 843 | Loss: 8.36255458125379e-06\n",
      "Iteration: 844 | Loss: 8.339653504663147e-06\n",
      "Iteration: 845 | Loss: 8.316835192090366e-06\n",
      "Iteration: 846 | Loss: 8.294106009998359e-06\n",
      "Iteration: 847 | Loss: 8.271460501418915e-06\n",
      "Iteration: 848 | Loss: 8.248903213825542e-06\n",
      "Iteration: 849 | Loss: 8.226428690250032e-06\n",
      "Iteration: 850 | Loss: 8.204041478165891e-06\n",
      "Iteration: 851 | Loss: 8.18173612060491e-06\n",
      "Iteration: 852 | Loss: 8.159514436556492e-06\n",
      "Iteration: 853 | Loss: 8.13737824501004e-06\n",
      "Iteration: 854 | Loss: 8.11532481748145e-06\n",
      "Iteration: 855 | Loss: 8.093352334981319e-06\n",
      "Iteration: 856 | Loss: 8.07146352599375e-06\n",
      "Iteration: 857 | Loss: 8.049652933550533e-06\n",
      "Iteration: 858 | Loss: 8.027932381082792e-06\n",
      "Iteration: 859 | Loss: 8.006282769201789e-06\n",
      "Iteration: 860 | Loss: 7.984719559317455e-06\n",
      "Iteration: 861 | Loss: 7.963236384966876e-06\n",
      "Iteration: 862 | Loss: 7.94183142716065e-06\n",
      "Iteration: 863 | Loss: 7.920505595393479e-06\n",
      "Iteration: 864 | Loss: 7.899261618149467e-06\n",
      "Iteration: 865 | Loss: 7.878095857449807e-06\n",
      "Iteration: 866 | Loss: 7.857004675315693e-06\n",
      "Iteration: 867 | Loss: 7.835993528715335e-06\n",
      "Iteration: 868 | Loss: 7.815062417648733e-06\n",
      "Iteration: 869 | Loss: 7.794204066158272e-06\n",
      "Iteration: 870 | Loss: 7.773424840706866e-06\n",
      "Iteration: 871 | Loss: 7.752726560283918e-06\n",
      "Iteration: 872 | Loss: 7.732097401458304e-06\n",
      "Iteration: 873 | Loss: 7.71155009715585e-06\n",
      "Iteration: 874 | Loss: 7.691074642934836e-06\n",
      "Iteration: 875 | Loss: 7.670673767279368e-06\n",
      "Iteration: 876 | Loss: 7.650348379684146e-06\n",
      "Iteration: 877 | Loss: 7.63009757065447e-06\n",
      "Iteration: 878 | Loss: 7.609918156958884e-06\n",
      "Iteration: 879 | Loss: 7.5898201430391055e-06\n",
      "Iteration: 880 | Loss: 7.569788976979908e-06\n",
      "Iteration: 881 | Loss: 7.549832844233606e-06\n",
      "Iteration: 882 | Loss: 7.529949016316095e-06\n",
      "Iteration: 883 | Loss: 7.510135219490621e-06\n",
      "Iteration: 884 | Loss: 7.490399184462149e-06\n",
      "Iteration: 885 | Loss: 7.470731361536309e-06\n",
      "Iteration: 886 | Loss: 7.451134024449857e-06\n",
      "Iteration: 887 | Loss: 7.431607173202792e-06\n",
      "Iteration: 888 | Loss: 7.412153991026571e-06\n",
      "Iteration: 889 | Loss: 7.392769475700334e-06\n",
      "Iteration: 890 | Loss: 7.373450443992624e-06\n",
      "Iteration: 891 | Loss: 7.354210538323969e-06\n",
      "Iteration: 892 | Loss: 7.335031114052981e-06\n",
      "Iteration: 893 | Loss: 7.315924904105486e-06\n",
      "Iteration: 894 | Loss: 7.296887815755326e-06\n",
      "Iteration: 895 | Loss: 7.27791939425515e-06\n",
      "Iteration: 896 | Loss: 7.259017365868203e-06\n",
      "Iteration: 897 | Loss: 7.240185368573293e-06\n",
      "Iteration: 898 | Loss: 7.221420219138963e-06\n",
      "Iteration: 899 | Loss: 7.202719189081108e-06\n",
      "Iteration: 900 | Loss: 7.184085916378535e-06\n",
      "Iteration: 901 | Loss: 7.165521310525946e-06\n",
      "Iteration: 902 | Loss: 7.147023552533938e-06\n",
      "Iteration: 903 | Loss: 7.128590368665755e-06\n",
      "Iteration: 904 | Loss: 7.110220849426696e-06\n",
      "Iteration: 905 | Loss: 7.091921361279674e-06\n",
      "Iteration: 906 | Loss: 7.073680990288267e-06\n",
      "Iteration: 907 | Loss: 7.055508831399493e-06\n",
      "Iteration: 908 | Loss: 7.037400791887194e-06\n",
      "Iteration: 909 | Loss: 7.019355507509317e-06\n",
      "Iteration: 910 | Loss: 7.001377070992021e-06\n",
      "Iteration: 911 | Loss: 6.983460934861796e-06\n",
      "Iteration: 912 | Loss: 6.9656070991186425e-06\n",
      "Iteration: 913 | Loss: 6.947816473257262e-06\n",
      "Iteration: 914 | Loss: 6.930087238288252e-06\n",
      "Iteration: 915 | Loss: 6.912421667948365e-06\n",
      "Iteration: 916 | Loss: 6.894820671732305e-06\n",
      "Iteration: 917 | Loss: 6.87727924741921e-06\n",
      "Iteration: 918 | Loss: 6.8597987592511345e-06\n",
      "Iteration: 919 | Loss: 6.842384209448937e-06\n",
      "Iteration: 920 | Loss: 6.8250242293288466e-06\n",
      "Iteration: 921 | Loss: 6.807730187574634e-06\n",
      "Iteration: 922 | Loss: 6.790492989239283e-06\n",
      "Iteration: 923 | Loss: 6.773316272301599e-06\n",
      "Iteration: 924 | Loss: 6.756200036761584e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 925 | Loss: 6.739142008882482e-06\n",
      "Iteration: 926 | Loss: 6.722146281390451e-06\n",
      "Iteration: 927 | Loss: 6.7052092163066845e-06\n",
      "Iteration: 928 | Loss: 6.688331268378533e-06\n",
      "Iteration: 929 | Loss: 6.671511982858647e-06\n",
      "Iteration: 930 | Loss: 6.654751359747024e-06\n",
      "Iteration: 931 | Loss: 6.63804621581221e-06\n",
      "Iteration: 932 | Loss: 6.621401098527713e-06\n",
      "Iteration: 933 | Loss: 6.604813279409427e-06\n",
      "Iteration: 934 | Loss: 6.588280029973248e-06\n",
      "Iteration: 935 | Loss: 6.571810899913544e-06\n",
      "Iteration: 936 | Loss: 6.555392701557139e-06\n",
      "Iteration: 937 | Loss: 6.539033165608998e-06\n",
      "Iteration: 938 | Loss: 6.522726380353561e-06\n",
      "Iteration: 939 | Loss: 6.506481895485194e-06\n",
      "Iteration: 940 | Loss: 6.490289251814829e-06\n",
      "Iteration: 941 | Loss: 6.474153906310676e-06\n",
      "Iteration: 942 | Loss: 6.458069947257172e-06\n",
      "Iteration: 943 | Loss: 6.442046469601337e-06\n",
      "Iteration: 944 | Loss: 6.426075742638204e-06\n",
      "Iteration: 945 | Loss: 6.41016049485188e-06\n",
      "Iteration: 946 | Loss: 6.394298452505609e-06\n",
      "Iteration: 947 | Loss: 6.378489160852041e-06\n",
      "Iteration: 948 | Loss: 6.3627348936279304e-06\n",
      "Iteration: 949 | Loss: 6.3470379245700315e-06\n",
      "Iteration: 950 | Loss: 6.331390068226028e-06\n",
      "Iteration: 951 | Loss: 6.31579587206943e-06\n",
      "Iteration: 952 | Loss: 6.3002557908475865e-06\n",
      "Iteration: 953 | Loss: 6.284769369813148e-06\n",
      "Iteration: 954 | Loss: 6.269335244724061e-06\n",
      "Iteration: 955 | Loss: 6.2539502323488705e-06\n",
      "Iteration: 956 | Loss: 6.238621153897839e-06\n",
      "Iteration: 957 | Loss: 6.22333982391865e-06\n",
      "Iteration: 958 | Loss: 6.2081135183689184e-06\n",
      "Iteration: 959 | Loss: 6.192939054017188e-06\n",
      "Iteration: 960 | Loss: 6.177815521368757e-06\n",
      "Iteration: 961 | Loss: 6.162740646686871e-06\n",
      "Iteration: 962 | Loss: 6.14771988693974e-06\n",
      "Iteration: 963 | Loss: 6.132744601927698e-06\n",
      "Iteration: 964 | Loss: 6.117824796092464e-06\n",
      "Iteration: 965 | Loss: 6.1029536482237745e-06\n",
      "Iteration: 966 | Loss: 6.08813115832163e-06\n",
      "Iteration: 967 | Loss: 6.073358690628083e-06\n",
      "Iteration: 968 | Loss: 6.0586348809010815e-06\n",
      "Iteration: 969 | Loss: 6.0439633671194315e-06\n",
      "Iteration: 970 | Loss: 6.029336418578168e-06\n",
      "Iteration: 971 | Loss: 6.014761765982257e-06\n",
      "Iteration: 972 | Loss: 6.0002371355949435e-06\n",
      "Iteration: 973 | Loss: 5.985758434690069e-06\n",
      "Iteration: 974 | Loss: 5.971327937004389e-06\n",
      "Iteration: 975 | Loss: 5.956946552032605e-06\n",
      "Iteration: 976 | Loss: 5.942610641795909e-06\n",
      "Iteration: 977 | Loss: 5.92832429902046e-06\n",
      "Iteration: 978 | Loss: 5.914083430980099e-06\n",
      "Iteration: 979 | Loss: 5.899890766158933e-06\n",
      "Iteration: 980 | Loss: 5.88574630455696e-06\n",
      "Iteration: 981 | Loss: 5.871648227184778e-06\n",
      "Iteration: 982 | Loss: 5.857594260305632e-06\n",
      "Iteration: 983 | Loss: 5.843592134624487e-06\n",
      "Iteration: 984 | Loss: 5.829629571962869e-06\n",
      "Iteration: 985 | Loss: 5.815717031509848e-06\n",
      "Iteration: 986 | Loss: 5.801850420539267e-06\n",
      "Iteration: 987 | Loss: 5.788028374809073e-06\n",
      "Iteration: 988 | Loss: 5.7742531680560205e-06\n",
      "Iteration: 989 | Loss: 5.760520707553951e-06\n",
      "Iteration: 990 | Loss: 5.746835995523725e-06\n",
      "Iteration: 991 | Loss: 5.733196758228587e-06\n",
      "Iteration: 992 | Loss: 5.7195966292056255e-06\n",
      "Iteration: 993 | Loss: 5.7060465223912615e-06\n",
      "Iteration: 994 | Loss: 5.6925368880911265e-06\n",
      "Iteration: 995 | Loss: 5.6790750022628345e-06\n",
      "Iteration: 996 | Loss: 5.6656572269275784e-06\n",
      "Iteration: 997 | Loss: 5.652279924106551e-06\n",
      "Iteration: 998 | Loss: 5.638948096020613e-06\n",
      "Iteration: 999 | Loss: 5.625658559438307e-06\n"
     ]
    }
   ],
   "source": [
    "cfm.fit(session_sparse_tensor, loss_func, learning_rate=200, debug=True, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "48142890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "ccf78610",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = cfm.get_embeddings\n",
    "user_emb, product_emb = embeddings_dict.values()\n",
    "user_emb, product_emb = user_emb.numpy(), product_emb.numpy()\n",
    "predictions = tf.matmul(user_emb, tf.transpose(product_emb)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "e6ffe54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOT = 'dot'\n",
    "COSINE = 'cosine'\n",
    "def compute_scores(query_embedding, item_embeddings, measure=DOT):\n",
    "    \"\"\"Computes the scores of the candidates given a query.\n",
    "    Args:\n",
    "    query_embedding: a vector of shape [k], representing the query embedding.\n",
    "    item_embeddings: a matrix of shape [N, k], such that row i is the embedding\n",
    "      of item i.\n",
    "    measure: a string specifying the similarity measure to be used. Can be\n",
    "      either DOT or COSINE.\n",
    "    Returns:\n",
    "    scores: a vector of shape [N], such that scores[i] is the score of item i.\n",
    "    \"\"\"\n",
    "    u = query_embedding\n",
    "    V = item_embeddings\n",
    "    if measure == COSINE:\n",
    "        V = V / np.linalg.norm(V, axis=1, keepdims=True)\n",
    "        u = u / np.linalg.norm(u)\n",
    "    scores = u.dot(V.T)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "2587ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_scores(user_emb, product_emb, measure=COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "989967e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products(user_id, k=100):\n",
    "    user_product_scores = scores[user_id]\n",
    "    user_locale_products = locale_product.copy()\n",
    "    user_locale_products['user_scores'] = user_product_scores\n",
    "    return list(user_locale_products.sort_values(by=['user_scores'], ascending=False).iloc[:k]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "8dcb4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(user_id, k=100):\n",
    "    reccos = recommend_products(user_id, k=k)\n",
    "    true_product = locale_train_sess.iloc[user_id]['next_item']\n",
    "    rank = np.where(reccos==true_product)[0]\n",
    "    if rank.shape[0] == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return rank[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "910a27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predicitons(locale, sess_test_locale):\n",
    "    random_state = np.random.RandomState(42)\n",
    "    products = read_product_data().query(f'locale == \"{locale}\"')\n",
    "    predictions = []\n",
    "    for _ in range(len(sess_test_locale)):\n",
    "        predictions.append(\n",
    "            list(products['id'].sample(PREDS_PER_SESSION, replace=True, random_state=random_state))\n",
    "        ) \n",
    "    sess_test_locale['next_item_prediction'] = predictions\n",
    "    sess_test_locale.drop('prev_items', inplace=True, axis=1)\n",
    "    return sess_test_locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a92010c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300690</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07XDY79PP, B0B33MVZ7C, B0B9HPZVHQ, B0948QWNQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232426</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07C8GM4H1, B000R32OKY, B0759VDBSM, 152476399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83969</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09CQBRGH9, B0B94DZZLH, B09TZVCKHK, B08DP33Y5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42591</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07W92BSY2, B0B7MH1MDP, B08CMPYH56, B07BN8LG7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171048</th>\n",
       "      <td>JP</td>\n",
       "      <td>[B002TV1GOM, B0039XRN2I, B09HRY7X6R, B0BGBS559...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "300690     UK  [B07XDY79PP, B0B33MVZ7C, B0B9HPZVHQ, B0948QWNQ...\n",
       "232426     UK  [B07C8GM4H1, B000R32OKY, B0759VDBSM, 152476399...\n",
       "83969      DE  [B09CQBRGH9, B0B94DZZLH, B09TZVCKHK, B08DP33Y5...\n",
       "42591      DE  [B07W92BSY2, B0B7MH1MDP, B08CMPYH56, B07BN8LG7...\n",
       "171048     JP  [B002TV1GOM, B0039XRN2I, B09HRY7X6R, B0BGBS559..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions = read_test_data(task)\n",
    "predictions = []\n",
    "test_locale_names = test_sessions['locale'].unique()\n",
    "for locale in test_locale_names:\n",
    "    sess_test_locale = test_sessions.query(f'locale == \"{locale}\"').copy()\n",
    "    predictions.append(\n",
    "        random_predicitons(locale, sess_test_locale)\n",
    "    )\n",
    "predictions = pd.concat(predictions).reset_index(drop=True)\n",
    "predictions.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429ce4d",
   "metadata": {},
   "source": [
    "## Validate predictions ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c73e0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_predictions(predictions, check_products=False):\n",
    "    \"\"\"\n",
    "    These tests need to pass as they will also be applied on the evaluator\n",
    "    \"\"\"\n",
    "    test_locale_names = test_sessions['locale'].unique()\n",
    "    for locale in test_locale_names:\n",
    "        sess_test = test_sessions.query(f'locale == \"{locale}\"')\n",
    "        preds_locale =  predictions[predictions['locale'] == sess_test['locale'].iloc[0]]\n",
    "        assert sorted(preds_locale.index.values) == sorted(sess_test.index.values), f\"Session ids of {locale} doesn't match\"\n",
    "\n",
    "        if check_products:\n",
    "            # This check is not done on the evaluator\n",
    "            # but you can run it to verify there is no mixing of products between locales\n",
    "            # Since the ground truth next item will always belong to the same locale\n",
    "            # Warning - This can be slow to run\n",
    "            products = read_product_data().query(f'locale == \"{locale}\"')\n",
    "            predicted_products = np.unique( np.array(list(preds_locale[\"next_item_prediction\"].values)) )\n",
    "            assert np.all( np.isin(predicted_products, products['id']) ), f\"Invalid products in {locale} predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56c2aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a01ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its important that the parquet file you submit is saved with pyarrow backend\n",
    "predictions.to_parquet(f'submission_{task}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2aa74",
   "metadata": {},
   "source": [
    "## Submit to AIcrowd 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462db565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can submit with aicrowd-cli, or upload manually on the challenge page.\n",
    "!aicrowd submission create -c task-1-next-product-recommendation -f \"submission_task1.parquet\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
